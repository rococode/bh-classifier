,recommend_why,I would get more accurate results doing it myself manually rather than using this current model.,I totally agree that it would make the job more simple to do and make my work less hard and I could get other things done ,I would still have to go through them to catch it's mistakes.,While it would be quicker, I fear accuracy may be low.,To many errors to be fully useful.  Some of the mis classifications were very obvious,I would feel more confident sorting through them myself, especially since I have an interest in baseball and hockey.,I would want to find a better model. Since this model did do very well but not perfect, I might want to adjust this model so that it was more accurate.,I think that it was accurate enough to go through mass emails with reliable results,It would be faster for me to allow the model to sort my boss's emails and then spot check than it would be for me to attempt to sort through thousands of emails one by one. The machine got enough of the emails correct that a few wrongs here or there in a batch of thousands is acceptable to me.,I'd use it because I don't see the model making a ton of mistakes.,Better it than me.,I think it would help speed up the process.,because its wrong too much,Since there were few errors i would use the model if there were a lot of emails and the sport was important to my boss.,It's a good tool to use.  It will save a lot of valuable work time.,it did pretty well,It would make the task of going through the emails a lot easier.,For more important stuff I'm still a bit unsure,The fault would lie with the person using the system so I would not want to have to recheck the work. I might as well just do it.,Same reasons as stated above.,it would save me the time of manually sorting them,I could not trust it to accurate categories the emails, so my boss would not be happy with the work.,it works kind of,I don't think it would save me much time since I would have to review it's work anyways.,I would only have to recheck to make sure all answers were correct before submission; therefore I would still be doing the work myself which would be time consuming,The model would be extremely helpful in sorting my boss's emails. For one, it would take me by myself a long period of time to sort a large amount of emails. The model was highly accurate in distinguishing emails by their respective categories. So if it cuts down the amount of time needed to sort my boss's emails, it would be unwise of me not to use the model.,It would be much quicker than sorting through the emails manually.  Manually sorting through the emails would rely on my own knowledge of both sports.  I would expect similar mistakes if I did the task manually since I don't know everything about baseball and hockey.,Too many mistakes ,I feel that it could make the work load lighter by providing the initial sort, which you could verify and clean up later.,It labelled some emails incorrectly
,how_decide,I think that the machine learning model took certain keywords and calculated them in some database. I think that there may be a database of certain terms, such as NHL, MLB, All Star games, etc. It would be wise to include player name databases I feel, too.,It seem like when it had the words hockey or baseball in them it did a better job and tell what they were, Sometimes I feel it went by the names of teams that stood out , the NHL or MLB , it was good at identifying by using these methods to pick which sports it was , it could be used in it could be used in football and others sports ,I believe it looked for keywords, such as baseball and hockey. Furthermore it looked for MBL and NHL. However it missed some of those, so it might have only been looking in the title instead of the full text. On the ones that there weren't clues for I think it tended to lean toward baseball, perhaps because baseball season is so long and there are so many games the AI thought that the probability was that the text was about baseball.,The keywords used in the emails is what I assume the model was using and it was fairly accurate but, I'd give it about 75% accuracy rate.,It looks like it decided mostly by what was described in the email.  Ie I think it used terms that were associated with either sport,At first I thought it looked for keywords and used names of players included in the email.  However, after seeing a few which were obviously one or the other by context clues, and seeing the machine got them wrong, I am not entirely sure about that anymore.  Perhaps the machine runs a search engine of all the terms it's been programmed to look for and it takes an average of which of those terms show up more often in order to try and decide which sport it was looking at.,The model must have recognized some keywords and judged based on whether most of the recognized keywords were related to baseball or hockey. It must be missing some keywords, however, because there were at least a couple of obvious mistakes.,I think the machine looked for key words that would be associated with either sport then decided to label it one way or the other ,It appears as though the machine learning model tried to study every word in the email. Some of the emails that it did not do well on was when the emails mentioned only team names. It did not seem to be able to differentiate between baseball teams and hockey teams. Mostly it did okay. I think that if I used this system, I would still have to do quite a lot of spot checking.,Depending on the content of the email. Does the email specifically use the term baseball and terms related to baseball. Does the email specifically mention terms associated with hockey. ,I think it acted on basic keywords and that would effect the accuracy.,Basically it looked for keywords such as hockey, baseball, nhl, mlb, the names of the conferences, puck, ball, and team names.,i think it decided by seeing HL or a different keyterm and it would decide.,WHEN THERE IS A SPECIFIC REFERENCE TO A PLAYER WHO IS INVOLVED IN A SPORT THAT IS A KEY INDICATOR. WHEN JARGON OF THE SPORT IN INCLUDED THAT IS A KEY INDICATOR. LIKE FOR BASEBALL HR Home run Hockey Goal or Goalie.
Mentioning a team city that has a team in the city. Mentioning the nickname of the team Baseball Cardinals Hockey Bruins. Using initials that identify a league like baseball MBL hockey NHL. ,I think if the email contained the word hockey or baseball the machine was correct most of the time.  I also think that the machine used the email subject title to decide which sport was being referred to. ,by keywords,It looked for certain key words about baseball and hockey.,I think it probably looks for keywords- like NHL, MLB, pitching, goalie, probably the names of the players and teams. Likely just a massive database of words that are categorized as either baseball or hockey. Whichever one it finds more matches of would be picked. ,It took key words such as puck or strikes, or team names and used them to decide. The ones it did not understand were where it talked about general or peoples preference that did not actually use sports known terms.,I would say it is correct about 75% of the time. If the emailes mentioned the wrods "hockey" or "baseball" it marked it correctly. Or if specific team names or leagues were mentioned it marked the email correctly. A few emails did not have these keywords and I was able to determine what the email was about with other keywords suck as "ball" or "goal" and the computer marked them incorrectly. This was what I observed. And I feel it is looking for certain keywords and maybe a few more keywords need to be added in case others arent mentioned.,it searched for keywords relevant to each category ,It was correct about 80% of the time.  I thought it was looking for key words, but there was one that had the actual sport name which it got wrong.,keyword searching mostly,It decided by scanning the email for key words that relate to each sport. For example if the words, pitcher, run, MLB were in the email it would decide that it was about baseball. If the words goalie, puck, NHL were in the email it would be about Hockey.,It picked out words in the text that may have been relevant to either hockey or baseball and then related the answer to it. Although I do not believe it is 100% accurate because there were at least 4 times when I believe it gave the wrong answer. Also it would be difficult to answer with no relevant words in the text or without prior knowledge or understanding of each of the games.,I think the machine learning model used keywords to decide whether emails were about baseball or hockey. Obviously, if the emails contained the words "baseball" or "hockey", the model would categorize the emails accordingly. But my hypothesis also extends to words related to each sport, such as "pitchers", the leagues associated with each sport like "MLB" or "AHL", or the teams that play in each sport. So the model uses words commonly used in each sport to make decisions.,I think the model is looking for specific keywords such as AHL, NHL, hockey, baseball, homerun, plus-minus, etc.  I also think it might be looking at city names and comparing known baseball and hockey cities. Lastly, it might be looking for famous baseball and hockey players' names.,Think it looks for keywords that are usually only use by one sport and not the other,It would pick up on key terms such as team names, the types of stats used, or on things that reflected the games they were talking about for example ball vs puck. ,Whether hockey or baseball was mentioned in the email.
,feedback_importance_why,I think that feedback is the only way to improve the model. If there is no feedback, then the machine knows not what it did wrong and will continue to make the same mistakes.,I think by letting the ones know that were putting this out there to be used that there would have to be some words in the email for it to make it ,We need to get it correct more often. It needs to learn from it's mistakes.,I would want to help improve it but, I would not waste a bunch of time on the improving of it if I had to double check it and waste my own time.,It might be useful if feedback was provided and used.  Program did not appear to be more than beta test ready,I would feel the need to check every email this machine sorted for approval.,It would be great if this model could be improved because it did very well, but there are some keywords that I think the model should recognized.,I think with feedback the machine would improve its reliability by having more data to go by and then would become more accurate as it went along,The more feedback you would be able to give this model the better it will perform in the future.,I think it's important to teach that model so that it can continually improve.,I think so yes,Feedback would just help the model do better in the future.,because i probably would be able to help it learn more.,If I saw errors that did not make sense I would want to be able to give feedback to improve the model and make the selections more accurate over time.,The feedback would result in the tool becoming more efficient.,to help them improve,There is always room for improvement so the chance to offer feedback is really helpful. ,Yes, because there could be cases you would notice it gets wrong and those need to be updated in the model,If I were trying to use it, I would need to be able to use it differently or find a way for it to be able to reject what it was unsure of.,Like my first reasoning, I could give feedback on certain keywords to be added for the computer program to look for.,the model wasnt correct every time so some corrections can be made and improved,I could add more key words for it to search for.,improvement is always relevant,I think it made some errors that were for emails that seemed obviously of one sport.,I would need to have the program fine tuned for greater efficiency,I feel like providing feedback to machine learning models is the only way that they are able to improve at working at tasks. If the model sorts through emails with no sense of whether it's right or wrong, it doesn't improve at all. It's the same as if I was assisted by a human with the task. We only learn and gain more knowledge by the mistakes that we make.,I would want to provide feedback to improve the model for possible future uses.  If there was no ability to provide feedback, I wouldn't feel comfortable endorsing using the model in the future since it would continue to make the same mistakes.,If my feedback was improving the bot right away sure I'll add feedback. If it's not helping me know it's just adding more things for me to do.,Continual approval is always good for technology, so being able to provide feedback would make for a better overall experience.,It labelled some emails incorrectly
,frustration_why,I felt that the model was inaccurate more times than I feel comfortable with. I feel that it is not capable of such categorization as of yet. ,I feel it would cut down on time spent on these and could really speed though them in no time , made the work faster and easier to do ,It made too many mistakes.,I would rather go through them myself so I know there are no errors. If I wanted to just look at hockey stuff and missed some due to incorrect sorting, I'd be frustrated.,I would still have to go through the emails.  The program appeared to be wrong at least 30%  whixh is rather high.  A useful program should be right at least 90 % of the time,I think the machine got at least 5 of the emails wrong, which is 25%.  That's not a very favorable margin of error to be using in a professional environment.,It is very uncomfortable for me to know there are errors in the data. I also think my boss might be frustrated if I turned in work with mistakes, even if there were relatively few mistakes like in this model.,For the most part the machine and I agreed on what the email was about and the few we didnt it was just part of the machine getting information wrong,I would prefer to have the model presort them for me. It would be faster for me to just skim through and make sure they were coded correctly than having to do it all myself.,I think the model did well.,I think it could work out well for that.,Even though im not well versed in either sport, I could easily separate the emails.,because it was wrong a lot,Since there is a chance for error I would not want to make a mistake with my boss so i agree slightly as long as there were only a few and every once in a while.,I think the machine learning model did a good job of sorting the emails.  I don't expect for the tool to be perfect.,It may make a mistake,I wouldn't be frustrated because the model was right the majority of the time. ,The model is pretty accurate so I would feel satisfied using it so I could do other work.,I would have to check the work of the model which would just make extra work when I could do it the first time.,Because there are some mistakes and I would probably be double checking anyways.,it would speed up the process for the majority of emails,It would be a waste of my time because it is inaccurate.,its pretty good,Because I think I couldn't completely rely on it and would have to go over it's work and check for errors.,I would have to recheck since there would be a 1 out of 5 chance that the answer would be wrong,I would feel rather relieved if I were to use this model to automatically sort my boss's emails. That would be a daunting and monotonous task to accomplish on my own. Using a model that has a relatively high accuracy rate to complete the task would take a weight off my shoulders.,I would only feel frustrated if the model was making a lot of mistakes.  I think the time saved by using the model justifies its use.,I'd have to go through them all anyways.,I feel that the model is right more often than it is wrong, and thus can be mostly counted on, with a little room  for oversight.,It labelled some emails incorrectly
,learn_why,I think that if the model takes into account my answers, it should learn in the process because I feel that I did not miss many, if even a single one.,I think it will learn each time it is used and make it more prefect in doing it task ,I didn't see any sign that it was getting better at it's predictions.,I think they were presorted so the information gained here would not be of much help.,I am not sure.  The program did not seem to learn from the emails as it classified a ltittle worse later,I don't feel like the performance improved over the course of the session, in fact I feel like it may have declined.,Machine learning models often learn as they go, so it's very possible the model learned from my answers. Of course, there's always the possibility that my answers confused it as well, or that that's not how this particular model works.,I think as it got more information it would be better at picking out keywords,I believe that the model begins learning from the first time that you either correct it or tell it that it was correct.,It seems fairly easy to understand why the emails were classified as such, thus I do believe the model can learn from the emails.,it depends on how it was programmed,Yes, I think with each email and batch the model improves.,it should have yes, cause i gave it feedback,I feel the model will adjust when errors are made and improve over time. I think there were minor improvements in this survey.,Yes, I think the model learned from the previous phase.  It probably has other steps to take when the answer is incorrect.,It was pretty accurate,I think it picked up key words from the emails and learned from them. ,No, we were just evaluating the model,Yes, I think it can continue to learn if someone programs on what it may have done wrong.,Same as I stated before,it seemed like it was answering correctly besides not sure the entire time,I don't know how it would learn based solely on my corrections of it.  I would need to provide keywords that it missed.,didnt seem real,It seemed like it made errors towards the end that if it were learning from previous emails it shouldn't have made.,It seemed to get more answers wrong as I progressed through the emails,With the information I have available, I think the model didn't really learn from the emails as far as what I saw. With emails that have no useful phrases or words to categorize them, it'll be difficult for the model to learn from those mistakes. Perhaps learning from the emails they categorized correctly will help it in being more accurate at the ones it got wrong. I'm not sure, but it is a machine learning model, so I'd be surprised if it didn't learn from the task.,I don't think it could learn anything from me giving my opinion on how an email should be categorized without giving any supporting evidence as to why I made my choice.,No idea,BEcause there were small terms that I don't think it would have picked up without some form of machine learning first.,It kept making mistakes
,accuracy_standard_why,I think mistakes generally aren't acceptable in a sense. Machines will make mistakes as they learn, but we shouldn't accept sub-par results. ,The emails did not have anything written in these to tell what they were about so it was a guess most of the time for the machine , ,The boss doesn't want mistakes. He wants a proper sort.,I do not expect the model to be perfected yet and some things it would be hard for it to guess due to slang or incorrect terminology used by the sender.,I think the alogrorithm could have been better.  In some cases it classified some obvious emails as the wrong sport.  Ie NHL for baseballe???,If someone is trying to be efficient with their work they need accuracy, especially if this person is a writer/editor for some kind of publication and wanting to get input from fans and/or analysts for their work.,Some of the mistakes were obvious, like when the emails talked about goals and goalies but labeled it as a baseball email. I think the model should mark an email it's unsure of as unknown so I could look at the unknowns and decide for myself. I think a human should do the task until the model can perform at least as well as a human at the task.,Because there is error in all aspects of life everything has a failure at one point or other,A model has to learn somehow. You must give the model feedback in order for it to get better at what it is trying to do. Eventually it will do better and make fewer mistakes.,I don't think a few mistakes will cause harm.,Is it acceptable for us to?,Well some things were kinda vague, like the email about the all star team/game. Even a person wouldn't really know how to distinguish that unless the email had more details.,because its just a computer,I think since the model has to pick on there are some where neither can be picked within reason so I agree slightly that you can accept some mistakes.,It is a machine and is subject to errors just as humans are.,it is not perfect.,Some of the emails are really vague and it's hard to tell what is being discussed so I think mistakes are guaranteed to happen. ,Im neutral on this because I don't think a few mistakes would make a big difference for the data, but it seems like a simple enough task that mistakes should be very rare.,You would not want the machine making this kind of error when dealing in business.  A 90 percent approval is not good enough in business.,Technology is made by humans. No one is perfect and technology is also far from perfect.,some have no words related to either subject,I would want to provide my boss with accurate information.  I could not completely rely on the model; therefore, I would need to do the work anyway.  It would not save me any time.,its impossible to be perfect.,There might be emails that are ambigous and may be about one of the sports but would not mention any keywords making it difficult for the model to decide whether it was about hockey or baseball. ,If I was given this assignment there would be a 1 out of 5 chance that the answer selected would be wrong and this would not be a good evaluation of my work by my superior who entrusted me to this assignment. If it had better statistics on its accuracy, then iot would be a good method to make the job quicker and efficient,I feel it's acceptable for the model to make a few mistakes because it'll sort through many emails and no model is perfect. Also no person would be able to sort through a large amount of emails perfectly either. I was unsure about one of the emails that the model may or may not have gotten correct. There were no words related to hockey or baseball contained in the email, only that it was about the All-Star Game. Both sports have an All-Star Game. So if I couldn't figure out how to sort the email, the model shouldn't be expected to either.,When dealing with large batches of information, I would expect a few mistakes even when done by a human.  The time saved by having a computer model sort through the emails justifies a few mistakes.,It's for my boos so I need it to do a good job. Any mistakes it does have will be my mistakes. ,There were times where the answer was incorrect/,It was designed by a human and humans make mistakes.
,overall,I think that it has potential and it did accurately identify some, however it needs work in accuracy as some were quite obvious with very obvious keywords that should not be missed. ,I like it , it was faster and I could see how it would make my job a lot easier for my self ,It was a little frustrating.,I feel it is a good concept but, once again, due to slang and unidentifiable terms, it would never be 100% accurate.,It could be good program but it was still to buggy.  Needs more work/,As I said above, it only gives me about 75% confidence.  Taking the time to have someone manually sort the emails would be worthwhile.,I thought the model did very well, but there are a few places that it could be improved. It has a lot of potential.,I thought it was overall better than not as far as it accuracy,I feel that the model was helpful to me. It will continue to learn and improve its function. I might also like the model to be able to put emails it isn't sure about in another folder for me to sort. That would help if it were able to decide that it couldn't say without doubt whether the email was about hockey or baseball.,It was fairly easy.,I think it was a good experience with high accuracy.,I felt it needs some work but overall it did a good job.,it was okay, def didnt really help much,It was interesting and fun because A! is the future so it will provide a lot of benefits in many areas. I also like hockey and baseball so it was fun to pick because I knew a lot of the correct answers and a few that could not be determined by the email.,I enjoyed working with the tool.  It was an example of using this type of tool to assist in the work environment.  It is probably widely used in some industries.,it was pretty accurate,It was a good experience. The model did what it had to do and it was interesting to learn a little.,It seems pretty reliable, though I think it missed one that was fairly obvious so that makes me question it a little bit.,I think it is advanced but until a machine can understand subtle differences in language or terms, it would not be good for this type of work.,It was ok. ,it felt like it was doing good and making things easier,It was not trustworthy.  It has potential to be trustworthy based on what it can learn.,gooe,It could be improved I wouldn't yet trust it to do accurate work.,It worked fairly well, but if I were to use it as a substitute for me doing the work, I would not have 100% accuracy which would be a negative evaluation of my work by my superior,I felt positive about the experience of using this model. It was smart in using words associated with each sports in sorting each email into the two different categories. It made some mistakes, but no more than the average human would in sorting through sports related emails they may not be familiar with. Knowing that it would be much faster at completing the task than myself makes me think of how useful it would be.,I thought the model was pretty accurate and would feel comfortable using it on a large batch sort.  I am open to using anything that makes a task more efficient.,I wouldn't use it, too many mistakes.,I felt that it was mostly trustworthy.,It was ok, but it did make some mistakes.
,perf_why,I think that a smart model would improve on feedback and not go backwards in accuracy. ,I think it would look for other things to use categorize the emails right ,It has nothing to learn from.,I feel there still needs work on the model and it will never be completely accurate.,Not really sure.  I would need to see another run,Again, I don't think the machine learned to be any more efficient as it went on with its work.,I don't know how much help my answers would be to the model, since I don't know a lot about hockey. I probably labeled at least one email incorrectly.,It would have feedback from the previous one and would add to its data set on how to decipher them,It is going to take awhile before the model is close to perfect. 20 emails in a batch of thousands doesn't amount to very much.,I think it would perform better because I believe it would have learned a lot more from the 20 emails.,I think it would work the same any time.,It would take what it did wrong, learn from it, and apply it to future trials.,because it should be learning as it is going,I think models learn over time and make adjustments so I feel the model will do better.,Again, I think that it is programmed to use other commands to get to the correct answer.,because there were some mistakes,I think more emails would be an opportunity for the model to learn more so it would make less mistakes. ,I don't know if the model works like that,If someone has programed it as too where it went wrong the first time, yes, I believe it could do better.,It depends on the emails it has to categorize and the keywords they have in them.,i couldnt tell if the model was improving or not ,Because I don't think it learned anything, I would expect it to be the same.,it was decent,I didn't feel like it was improving over time so I think it's preformance would stay the same.,It can only produce from the information it is programmed to decipher. If no changes were made to the program, it would produce the same results,I answered this somewhat before, but I feel like the model's accuracy would improve based on the emails it got right as well as those it got wrong. But it depends on the information it has available about the emails it's been shown before. If there's no clear reason or data shown why it got an email wrong, it wouldn't be able to categorize them any better in another set.,I think it would still make similar errors until more useful feedback is given when the model incorrectly categorizes an email.,Machine learning can get better the longer it runs,I think that it would take the information from my experience and apply more scrutiny.,I don't know if it knows it is wrong
,trust_why,There were some emails that seemed pretty obvious. Certain keywords such as hockey, NHL, etc were overlooked that should be simple to pick up on.,on most of them I feel that the machine would get 90 percent of them correct , ,On some of the obvious answers it missed and picked the wrong sport.,I would be more accurate than the model.,The algorithm was not accurate enough of the time so I would not really trust it.,As I said above, the accuracy is not high enough to have a standard of excellence.  It got the obvious ones right, but if the email was even a little bit obscure, the machine was wrong at least 50% of the time.,The model mostly chose the right answers, but there were still a few obvious mistakes.,For the most part it was accurate so it would be somewhat reliable to categorize emails,I would trust this model much more after I was able to give it some feedback and teach it to better differentiate between the two sports.,I think the model did well.,I think it would have a high accuracy rate and would be god to use for this.,It did a good job for the most part, so I think it would separate most accurately with a few mistakes here and there.,because it kind of can, but then again it kind of cant,I feel agree is the right answer since there were a few errors but not so many my boss would not be benefited.,The percentage of correct answers was very high.,it did pretty well,The model did a pretty good job deciding which one was baseball or hockey. There was a few mistakes but overall the majority was correct. ,I would trust it though I wouldn't expect perfection,Because I felt that it did not get them all right, and a human would have a better success rate. It did not understand the terms that were not sports only.,I would end up double checking them anyways. So I might as well do them in the first place.,it got most of them right,It makes obvious errors.,its close enough,I feel like it made enough errors that I would not completely trust it.,It gave most of the answers correctly but I would not be sure it would be 100% accurate,Most of my boss's emails about hockey or baseball would contain keywords within them that make it somewhat obvious what the person or persons are talking about. I only saw a few mistakes by the model during the course of the twenty emails shown. If the model is correct most of the time, I have no reason not to trust it.,I wouldn't fully trust the model since it made some errors on the trial, but it still categorized the majority of the emails correctly.  Even if I did it manually, I would expect some errors.,It made a decent amount of mistakes ,I think that after a few tweaks it would be able to most likely nearly perfectly sort the emails since it did a really good job of flagging them so far.,It labelled some emails incorrectly
_feature,recommend_why,after choosing more keywords, the machine will be even better.,It would make the task quicker especially if you can teach it to sort other topics,It was correct most of the time.,The model is useful and fairly accurate.,It woud be easier than manually sorting them.,I would have to double check them, so the model doesn't really help. ,I would feel it wasn't reliable enough.,This seems like a time saver. Again, I don't consider this any sort or imperative task. I would rather do something more useful.,It would at least save me a fair amount of time. It seems accurate enough to rely on it...especially if it will adapt to changes. ,Using the model to help sort my boss's emails would save me time and effort.,I felt the model was fairly accurate and think it could definitely assist me in my task.,I would have to find out first what the accuracy must be. If it is absolutely 100 percent, then no, I would not use it because of the one mistake I caught. ,If it does mess up, it wouldn't be for anything important. ,It would make me look bad if things were incorrectly sorted.,If it was more accurate,It would save time. ,I trust it to work well most of the time. ,I might give it a chance from time to time, but would more often trust my own work.,It would make things easier no doubt, but not  easy enough to just set it and walk away.,I think it would be helpful to do an initial sorting for review.,It still would make things easier. A quick glance to make sure it was correct.,i feel like it would be okay to use it but i trust myself more,This would certainly help me to split them between the two sports and it's accuracy. ,It's a good tool to use but it's not perfect. ,LIKE I SAID FOR THE MOST PART IT WAS ACCURATE SO YES I WOULD USE IT.,I probably would. I didn't mind sorting them manually but, I'd give this a chance,To get through the volume of emails being suggested, technology would make the process feasible. Otherwise, the time involved would be too much. I'd possibly incorporate some sort of auditing function.,I don't like mistakes. ,The model was very accurate.,I still think if I had to sort thousands it would be better than nothing. It was correct the majority of the time.
_feature,how_decide,It was looking for the keywords in an email that are associated with either baseball or hockey.,I think it picked up on certain words or athletes names,It decided based upon key words in its database.,It looks for key words that could point to either baseball or hockey,I think that it had a list of key-words that it used.  There were a couple that I am not sure about since they didn't really reference either sport and I saw nothing in them that would have tipped off the machine.,The model was usually right. It decided based on keywords that were in the email. ,The model was picking up on keywords more strongly associated with one sport or the other.  It actually did a pretty good job overall and didn't seem to get false results from over general words like game or score.  It seemed to have trouble if more than one type of game was mentioned, with one game being the focus and the other only coming up incidentally, it could find difficulty in determining which one was the main topic.  Interestingly, this seemed true even if the secondary game was something else like football.,I believe it looked at keywords and then sorted from previous right or wrong answers.,I have no idea how it decided, really. It was wrong on a few that were really obviously the other one. Apparently it picks out words that are more associated with one or the other...but I can't tell what it might've been picking up on. ,The machine learning model decided whether email were about baseball or hockey based on the presence of certain words.,I am honestly not sure, but I am assuming utilizing different key words that have been tagged by the creator of the model.  ,It had keywords to look for. If the majority of the keywords were in the hockey category, then it chose hockey. The same goes for baseball.,Looked for names of teams and players and terms unique to each sport, like home run, goalie, pitcher, etc.,It evaluated the overall text to determine if any keywords related to either sport,The machining learning model looked for key words and their prevalence as they relate to hockey or baseball.,It used certain words that belonged to each sport. ,It picked out keywords that could really only be associated between hockey and baseball. ,It appeared to be guided by some words and parameters, but did not understand some specific abbreviations and nicknames that were key.,The machine learning model picked up on certain words and decided that the email was either about baseball or hockey based on those sets of words.,The machine looked for key words associated with the different sports in the text of the email and made a determination based on those words.,The machine looked for keywords within the text to aid in determining which sport.,They took certain key words from articles and decided if it was most likely based on baseball or hockey.,It used player's names to determine which sport the emails were about. It also may have used the names of the teams to help. ,I believe it was decided using key words such as "pitch" or "goal". ,THE MODEL USED SPECIFIC WORDS GIVEN TO IT AND SCANNED TO SEE IF THE WORDS WERE INCLUDED IN EMAILS AND DECIDED THEN IF THE EMAIL WAS ABOUT BASEBALL OR HOCKEY.,It looked for key words that were specific to the individual topics (hockey or baseball),The machine model looked for words that were unique to either baseball or hockey and then assigned the topic of the email based upon the presence of those words.,Key Word Usage ,The machine learning model classified several key words that are only associated with each goal. For the most part the AI was accurate because the two sports have different strategies and names for the object in motion (ball/puck).,I think it used words in the email such as hockey and baseball and some names maybe to decide. It didn't seem to do as well when specific words were omitted. When puck was the only thing available it wasn't always right where a real person would know it was hockey. 
_feature,feedback_importance_why,I want to add more keywords,reporting feedback is the only way to help make it work better,It made a few error which should be easy to correct.,It is always good to have the opportunity to improve the model especially since the model is not perfect and can make mistakes.,If there were things that I could add to make it more accurate, I would want that chance.,Feedback could result in more accurate results. ,Maybe my feedback could improve it and then it would be more useful.,I think it continuously should be improved so its accuracy creeps up toward 100 percent overtime. ,It still got some things wrong...so being able to tweak it a little would hopefully help correct that and make it more efficient. ,The model appears to perform with a reasonable degree of accuracy.  However, providing feedback could result in improved accuracy.,While using the model I would want to be able to provide feedback and understand the reasons why the model chose certain selections.,If I can provide feedback that is utilized, then I would absolutely use it because mistakes would lessen considerably. ,Fine tuning it could make it even more useful. ,It would only become useful if I could teach it exactly how I would sort the emails.,To improve accuracy,I think it would learn from the feedback and to categorize certain words in it's database. ,Because if I caught a mistake i'd want it to be able to improve so it works better next time. ,Feedback or reprogramming is the only way it would improve.,There is definitely room for improvement which is why I would support feedback.  ,Identifying more key words would help strengthen the model.,There should always be feedback to improve things,needs to be made perfect!,It is very well programmed already. I don't believe I would have a need to provide feedback as there isn't much else that could improve it's % of accuracy. ,Providing feedback can only help in the long run. ,IF IT IS GETTING THE JOB WRONG I WOULD LIKE THE ABILITY TO IMPROVE IT BY PROVIDING FEEDBACK ON HOW IT CAN PROVIDE MORE ACCURATE RESULTS,I think it helps machines to learn when humans provide the training/feedback,As you see mistakes that the model has made or terms that would clearly sway it to one topic or the other, it would be important to be able to build it into the decision-making. Continuous improvement is really important in almost anything.,It needed improvement. ,I like the idea of being able to add additional keywords associated with each sport to help the model determine the correct classification.,Absolutely as like I said sometimes simple things were labeled incorrectly and just focusing on those words could make a huge improvement.
_feature,frustration_why,I trust the machine since it seems accurate.,It would make it easier,It's easier than doing it manually.,The model has a good record with a high percentage to correctly sort the emails,It would be better than reading through all of them myself.,I'd still have to check them anyway. ,The failure rate would mean I couldn't rely on it.,I believe it is mostly correct and will improve. Maybe I can have it flag vague emails and then can manually look at them.,It seems like it would get most of them right...so it'd be fine enough, probably. ,This model appears to sort the emails with an acceptable degree of accuracy.,I would probably use this model, but then have something that provided the unsure ones for me to review manually.,It had the majority of them correct from what I can tell. ,The only unclear one was a very vague e-mail. ,I would not like for it to make mistakes.,The model would be helpful but I would need to double check its results,I think overall it did a good job and I'm sure would save some time.,I think it would be very useful and not frustrating,I would hope a model would help aliviate some of my work load, but if I still had to check the results, it would make me question why I didn't just do it myself.,I would feel frustrated because it does seem to have a few cases of being inaccurate.  This would get old fast.  ,I would view this as a helping tool, not as something that could solve the problem without error. Therefore, I would not find it terribly frustrating.,I think it kept up pretty well. There were some emails that had no clues.,it worked pretty decently with a few mistakes,It was very accurate in most cases and it would eliminate my need to have to sort through so many emails my boss is requesting information for. ,It's not perfect. ,IF ITS MAKING MISTAKES YES, BUT IF ITS MOSTLY ACCURATE I WOULDN'T MIND USING IT ESPECIALLY IF I AM DOUBLE CHECKING IT ANYWAY.,I don't think I'd feel frustrated at all. ,There were only a few instances where the model clearly appeared to get it wrong when there were indicators present that pointed to either hockey or baseball. That would have been a little frustrating to observe that the model couldn't have figured that out.,I don't think it was too accurate, and found mistakes. ,From what I could tell the model was very accurate.,Many of the more obvious emails were right although I would be frustrated if the word puck was said as baseball as it would make me look stupid.
_feature,learn_why,I tried to choose carefully and I think I did a good job,I think I highlighted some words to help it learn,I think my suggestions would be helpful to it.,I picked good keywords to associate with the correct categorization for each email. ,I don't know if it learned from them, but I hope it did.,I chose words that it should only associated with baseball or hockey. ,There were several emails that weren't baseball or hockey related, but no option to mark them as such and move on.  I was forced to select 3 words any way.  That feedback is actually going to mess up the algorithm, not improve it.,I think probably on the couple of emails I corrected it on, it probably will incorporate the keywords into its algorithm and continue to improve its accuracy. ,I have no idea...I thought it said it wouldn't process them until we had done all 20...so I wouldn't have seen any changes to know if it did learn. ,I don't know. I haven't been given any information upon which to base this decision.,I think I provided some good insight, but even know that I screwed up on categorizing a few emails. ,I honestly have no way of knowing this since I don't know how interactive it was. If it was taking my answers and incorporating them into the model in some way, then yes, it would learn as it went on.,Not sure. ,I gave it good keywords that made sense, whenever I could.,It seemed to improve its results,I think it would learn because hopefully it added new words to it's process.,I assume it would since it makes sense to me. ,It sounded like the words chosen would be incorporated into the algorithm and might help it to improve.,I believe that it held on to new knowledge, unfortunately some of the emails didn't have enough options so it didn't make the model full proof.,I feel I added words to the model's list that were uniquely associated to one sport or the other.,I think there were enough words picked to help the program continue to get better.,i used pretty good keywords to distinguished them.,I believe that the model was set up well and there was very little I could have given for information to make it more accurate. ,I think it could only help to have given a human perception to help it learn. ,I THINK IT MAY HAVE I THINK I ONLY NOTICED ONE WRONGLY LABELED EMAIL SO IT MUST HAVE LEARNED FROM MY PREVIOUS RECORDINGS.,I think there were a few that the machine had wrong that were obvious to me, either from a popular player's surname or something similar. My highlighting those clues and "teaching" the machine probably helped,I believe I was able to identify some unique words in the emails it had miscategorized that would help it identify the sport better the next time they were encountered.,It better through the 20 examples. ,The keywords I provided were clearly only associated with each respective sport.,Just a few simple word changes would make a big difference even in the few emails I saw they would have been correctly sorted after those few changes.
_feature,accuracy_standard_why,I have seen some obvious hockey emails that are labeled baseball and vice-versa but the accuracy was about %90.,It hasn't been perfected,It's understandable if it makes a mistake when the email is ambigous.,Can't expect the sorting to be perfect.,There's no way to make it perfect.,No model is perfect and the human eye is always better. Then again accuracy is desired. ,I understand it's not perfect, but if I have to go through everything to check its mistakes, have I teally saved any time?,I don't consider this a task of paramount importance. It is not life or death or ostensibly something that appears to have any real consequences. I think the accuracy of this algorithm is pretty decent, and probably will continue improving. Being 80 percent correct is probably good enough.,There are a few situations when it's really ambiguous and could be either without any more information. ,if the model makes a few mistakes that is okay.  I would rather deal with correcting those mistakes rather than having to do all of the sorting myself.  This would still save me time.,I honestly was confused on a few of the emails and couldn't tell if they were about hockey or baseball and some words between the two can be shared, so it would be hard to catch every case 100% accurately.,Well, if it makes mistakes, then it is really not doing what it should do unless it can learn from its mistakes. If that is the case, then it would be okay to make a few.,Not a super important task. ,The model is only as good as its programming.,It would be understandable to make mistakes as a human cannot often tell if an email was about hockey or baseball,It's not acceptable because it would waste time if my boss was looking for just hockey emails or the other way around. ,Because it isn't perfect and it will occasionally mess up,Any quick sorting by human or machine would likely lead to a few mistakes.,Even tho it is  a sophisticated form of technology, we still have to allow for margin of error, at least in the beginning stage.,I feel that since there is overlap in terms, lots of variations including abbreviations and slang, and the fact that machine is not great at determining context all are good excuses for the occasional mistake.,Because I feel like there will always be mistakes and that it's better to assume mistakes to keep you are on top of stuff.,not everything is perfect,Nothing can be perfect in most cases and in some instances there was nothing specific in the email that would help determine which it was about, hockey or baseball. ,It's not perfect and sometimes there are no good key words in the email. ,IT DEPENDS ON HOW OFTEN IT MAKES MISTAKES, I AM SURE A HUMAN WOULD MAKE MISTAKES TOO.  BUT IF A HUMAN DOES IT BETTER THAN A MACHINE THEN THE HUMAN SHOULD HAVE THE JOB.,There were a few emails that weren't entirely definitive. One in particular, gave no clues regarding the topic. The writing simply asked how one might acquire All-Star tickets. Both hockey and baseball have all-star games so, there was a 50/50 chance of making a mistake,While there are many words that uniquely distinguish between hockey and baseball, there are some words that can be used interchangeably. It would make it difficult for even a human reader to determine which sport is referred to in some cases. Also, when dealing with a large volume and where speed is important, a few mistakes has to be accounted for in that situation. It's an acceptable tradeoff,I think if you are going to pay for a model, it should be more accurate. ,Sometimes only names are mentioned and this makes it difficult for the AI to tell the difference. It is acceptable if the program does not have a 100% accuracy level. ,I could go over the emails at a glance to double check. Or I could take a sample and see how the model did and correct any errors there.
_feature,overall,it was fun and accurate.,It seemed simple and easy enough,It was fine.,great positive experience,I think that it worked well most of the time.,It was pretty good overall. ,It was okay, but just ok. I expected better results.,I think just about anything that saves time on an insignificant task is good. This model would allow me to do work tasks that are more important.,It was fine. It seemed to catch most things, at least. It might help to see how it ultimately responds to the words I was inputting.,I feel comfortable and satisfied with the experience of using this model.,I thought it was very good other than it seemed slow and I had to click a few times to get the pages to fully show up.,I thought that it was very good. If I had to sort thousands of emails, I would much rather use this model!,I felt like it would be a useful tool and was impressed with its accuracy. ,It was kind of fun to show it my thought process, to a small extent.,It was helpful,It was okay.,I felt really good about it and it was interesting. ,It didn't bother me to use it, but I definitely felt I would have better results than the model.,I thought it was pretty fun and I can see this being a useful technology.,I found it very interesting and enjoyed working to improve the model.,It did well overall.,it was a really good model. it had a few problems but that can be fixed.,It showed how helpful the models can be made to make the job easier and get data needed quickly. ,I think it was interesting.,IT WAS EASY AND FUN.  I LIKED THAT A MODEL COULD DO THAT JOB ACCURATELY ,I enjoyed it.,I was pleasantly surprised at how well the model sorted the emails. The technology platform was a little clunky and it took me longer than I expected to get the pages to load, but overall it was a good experience.,It was ok, but not accurate enough. ,I enjoyed using this model because it was accurate.,It was pretty simple and easy to use. And with the improvement with words to focus on I think it could improve immensely.
_feature,perf_why,the more data, the better.,it needs more time and words to help it get better,It should have learned to correct some obvious mistakes.,For it to improve drastically I feel it would need more examples than only a set of 20 to improve.,I think the words that I picked out would help it be more accurate.,It learns a little more each time. ,It will get a little better from the baseball and hockey feedback but maybe a little worse because of the non hockey and baseball emails that were included.,I think it takes time to improve. I think the improvement would be negligible from one round of learning to the next, but over time improvement would be significant. ,I would think the words I gave it would help differentiate a little better. At least most of the time...some of the examples didn't have 3 words that really would and I was forced to just pick something random. ,The accuracy was already at an acceptable level.  I don't believe that there is room to see a significant improvement given a small set of emails.,I think it would perform a bit better based on my feedback.,I just don't know if it is using my feedback to make better choices. ,I don't think my tags really added much. ,It has learned more of the keywords to look for.,It would better correlate to key words,Only because there was one email that wasn't about either sport and somehow it brought it up as hockey so it would need more to learn. ,If it took my suggestions, then it should definetly work better since I was able to pick out good keywords. ,With my added feedback, it should have more specific patterns to work with.,I think it would do better because there was some blatant incorrect sorting that could easily be fixed by the new knowledge gained by the first 20 emails.,I feel like I identified some key terms that would do a good job at distinguishing between hockey and baseball.,I'm sure there would be some improvement,maybe not a lot better but definitely better. ,I believe it has almost all the information I provided available to it and my responses wouldn't increase it's accuracy.,I think it could incorporate my feedback well.,IF I AM GIVING IT CLEAR WORDS THAT ARE EITHER BASEBALL OR HOCKEY AND CANNOT BE CONFUSED BETWEEN THE TWO I THINK IT WOULD DO A BETTER JOB GOING FORWARD.,I think it learned a few things,There were some key words (e.g. ice, pitch, etc) that highlighted which sport was being referred to and hopefully they would clue the model into what decision to make better the next time.,It seemed to improve. ,The model was accurate to begin with, but with additional keywords it will be even more accurate.,I think it would catch some of the simple errors made previously as long as it does sort by words in the text as well as subject lines.
_feature,trust_why,I trust the machine since it seems accurate, more than %90,The model seemed to behave with decent accuracy,It seemed mostly correct.,The model has a good record with a high percentage to correctly sort the emails,For the most part, what I was mostly sorted correctly.,The model was usually right but made the occasional mistake. ,I would be worried that my boss would be upset by the failure rate.,It appeared to be mostly correct in the 20 emails I looked at and I bet I could continue to improve its accuracy.,With some tweaking...it could be better...but it was already pretty close, for the most part. ,The model for the most part identifies emails correctly.  It may make some mistakes.  Yet, there is also no guarantee that I would not make some mistakes on my own, especially with a large number of email to sort.,The model was mostly accurate, so overall I would trust it to perform the correct categorizations about 90% of the time.,It had the majority of them correct, though I did see one that was miscategorized. ,It seemed to work well with almost every normal email. ,It did not get everything correct.,For the most part it was correct but not always,It did make a few mistakes so I would have to double check the work but most would be done so it would save me time. ,Its right enough of the time where it would make things simpler for me. ,Mistakes were made in the previous examples, so I do not trust the model completely yet.,I would trust it to get the overall gist of the task, but I would still expect to  go over it to make sure nothing slipped through the cracks.,There is lots of room for error, so I would still feel the need for human supervision over the machine.,I would mostly trust it. It wasn't 100% though,it was mostly right with a few exceptions,It has been programmed to make the correct choices and it has done well in these 20 emails provided. ,It made a bunch of incorrect choices. ,FOR THE MOST PART THE MODEL WAS ACCURATE SO I WOULD USE IT.,I'd like to play around with it some more before making that determination. For the most part, it seemed to be doing a good job.,For the majority of emails I read, the model was correct. I would be somewhat uncomfortable, though, if there were really critical emails that could have been miscategorized,Mistakes in the examples. ,The model was very accurate.,It was inaccurate about 30 percent of the time.
_instance_explain,recommend_why,Human would be better,Because some of the results were incorrect. I only want 100% correct results,The model would speed up the selection process.,It would be beneficial to me and I wouldn't need to sort through as many e-mails.,Not the way it is right now. Maybe after a few more tweaks.,It didn't seem accurate enough because it ignored some obvious keywords in the emails.,I wouldn't want to manage without it after seeing the way it performs. There is no logical reason to do extra work after seeing the way it performed.,It would make the process easier and quicker.,It would cut down on manually sorting but I would have to still look to catch the errors.,The accuracy of the model is not sufficient and does not offer enough in way of providing feedback.,I would not want the chance for things to be sorted incorrectly.,I would be afraid an error would slip through.,I would prefer to read the emails myself.,I could do a better job than the model and I would not want to risk my job by relying on the model.,Because I got better things to do and it already performs pretty good. Boss won't know the difference and I'll go back and check during a slow time.,It will be helpful to make things go faster. ,It is time saving.,It's not very good.,The words it uses to distinguish between hockey or baseball could be used for either sport.,Like I said, it was just easier to have it presorted and then me go over and check to make sure it was done correctly. I felt like it just helped a lot more in the process, even if it did make a few mistakes.,It has a low accuracy ratings in my opinion.,Especially because there are so many emails, at the very least it could give a rough sort that I could do maybe a 2nd sort.,Better a human should do it.,Again the greater majority of the email was sorted correctly and it would definitely make it much easier for me to continual sorting.,I would use it once its errors are less frequent or eliminated but it's not there yet.,I don't trust the machine to sort the emails correctly. The machine was getting close to 50% of the emails wrong, so it's too untrustworthy to use at this moment. My boss will blame me for any issues that arise so I'd rather just do it myself.,I would worry that it would not get some things right.,I thought it had too many inaccurate answers, as I expect it to be 100% right all the time.,If there was nothing else to use, ok. but it could definitely be improved upon.
_instance_explain,how_decide,If it had baseball or hockey in it. If it had the teams or sports terms.,It seemed to be looking at only technical terms, nothing that I would feel completely confident on relying upon. It seemed to look at words that described players and such. ,The model seems to try to pick out players' names, which would pin down the type of game and even the team. It only sometimes notices the actual name of the game in the text. I think the largest problem with this model is that it too often uses random words as base words, entirely not noticing exact words which would determine the game perfectly.,The model mostly found terms that describe actions such as "pitch" and and objects such as "puck." However, it would have been more beneficial to prioritize the terms to include simply "baseball" or "hockey." It could've also included MLB and NHL.  Generally, the terms highlighted were very vague and could have applied to either baseball or hockey.  Nonetheless, it was fairly accurate.  This makes me think that the model found how the various terms were connected to one another and then probably used probability.,I don't know how the machine decided. It looked like it was highlighting things at random. ,The choice of keywords seemed a little haphazard, but in a few cases it did seem to pick words that made the decision obvious.  There were many instances where obvious keywords were ignored, so I'm not certain how the algorithm works.,The machine learning model processed the text and highlighted three keywords most important to its decision making between either hockey or baseball.,It made note of keywords primarily.  If one does that, the answer becomes pretty obvious most of the time.,Sometimes it seemed like it was focusing on one word that could be associated with hockey, like cup, when cup actually referred to an actual styrofoam cup. Another thing that stood out was the All Stars email. I had a hard time judging that one but think the model was incorrect. It seemed like it did pick up on the keywords that were more obvious.,Overall, the machine model was not clear in outputting how the choice between Baseball or hockey was being determined. The words shown to be highlighted in yellow were nearly entirely of little to no relevance in making categorization. Very rarely, if every, were what I considered relevant key words highlighted such as: MLB, NHL, AHL, hockey, puck, goal, hit, home run, base, ice. ,It looked at keywords to determine if they were related to baseball or hockey.,It looked for certain keywords that might be related to either hockey or baseball and made its decision based off that. ,Based on highlighted words.,The model reviewed key words in the text to match for known words related to the sports in question.,I guess it develops an algorithm based on the key words and which sport they indicate after a number of attempts and going through a learning process.,It focused on key words. But some of the key words that were highlighted had nothing to do with hockey or baseball. ,It found key words, some obvious and some less obvious. For example, "baseball" or "hockey" Then there were other ones like "pitch" for baseball, and others for hockey. ,The model had a list of keywords that it would attempt to identify in the text it parsed. If certain ones appeared, it would classify it as one or the other.,While the machine was able to distinguish between baseball and hockey emails most of the time, it seemed that it more more of a guess on the part of the machine. It was using words like "most" and "the" which wouldn't be able to really know whether or not the email was about baseball or hockey. It should have been looking for the words that I was looking for to interpret the email, like the names of teams and distinct words that are used to refer to game play.,It used key words like goal, pitch, score, hit and had them sorted to baseball or hockey and skimmed for those words and made the decision that way. Maybe it took names as well.,It looks at the yellow words and decided that way.,I think it probably has a database of words that correlate to either hockey or baseball. If a word matches either of the databases then it will be more prone to selecting that sport. I think the model does run into problems when there are not any words to even try to rank in terms of how correlated they are. At that point it looks almost random.,Honestly? No clue! Quite often, the highlighted words were utterly irrelevant. Yet the thing seems to be about 75-80% accurate. It's a mystery to me.,The machine learning model seems to use key words that are related to either baseball or hockey in order to make a decision as to which category each email comes from. Many times there were clues in the email specific to baseball or hockey such as the words baseball or hockey or pitch or puck etc., and or the names of specific players or teams or team locations.,It tried to choose words from the email that would be more commonly used in each sport.thia worked for some sport specific words like goals or runs,etc. But didn't work very well when it chose arbitrary words or words that could apply to either sport like cities or names or unimportant non sport related words.

,The model seemed to lean towards predicting baseball unless certain keywords came up in the email, words like 'NHL.' It skipped over very important keywords though, including 'baseball', 'hockey', 'puck' and different locations which could help differentiate between baseball and hockey. I'm honestly not sure how or why the machine locked on to certain keywords that had nothing to do with the sport while skipping over extremely relevant words.,It used common words for each type of sport such as cup or homerun to decide which type of sport was being referenced.,Key words that were related to either in the emails.,They highlighted three words that were judged to be most pertinent and most likely to be about either baseball or hockey.
_instance_explain,feedback_importance_why,Everything needs fixing.,Because there may be new terms or nomenclature that would help improve the success rate ,Every program is open to corrections to improve it performance.,Improving this model makes it even more effectively beneficial for future use.,Besides thinking everything should have the ability to provide feedback, I think it could be useful if tweaked correctly.,It would be great if the feedback could be given by pointing out relevant keywords.,Using the feedback to improve, it is likely that the percentage of mistakes could be reduced considerably.,Being able to provide feedback is always helpful and often necessary to keep the system properly working.,Bugs and errors are found through trial and error and I would want to provide feedback so that keyword understanding could be improved.,The amount of feedback currently allowed to be entered (yes or no) is insufficient and must be expanded to improve the accuracy of the model.,That's the only way it can learn and become more useful.,Because if I could tell what words it missed or got wrong then it could be fixed and improved.,Feedback can help improve the model.,Learning and becoming better at a job is very important in any circumstance.,It has to learn from past experiences and develop some correct data which can only be done with some input. English is hard.,It definitely needs to be improved. So feedback needs to be able to be submitted regarding the models performance. ,I think it would be beneficial for teaching it to not make the same mistakes repeatedly if you can correct it.  ,An improved model would make my work more accurate to my boss, increasing the likelihood of a raise or promotion.,The machine would need to be programmed for my specific searches because it is not already.,It did a lot of things correct, but it would be nice to be able to correct the things that were wrong so it would work better for me.,If I can't give a feedback and have it use my feedback to improve then it would keep making mistakes.,Absolutely, there are a bunch of ways it could be improved and increase accuracy. I noticed it uses names of cities but sometimes one city has both hockey and baseball. That could be improved.,Feedback is important to any model.,Room for improvement is always good to have available whether doing a task on one's own or with machine technology assistance.,I would feel more comfortable teaching it the errors it's making so it can improve.,Hopefully my feedback would be incorporated into the model so that it would become more accurate and more trustworthy. The model isn't accurate enough right now so maybe it would learn from me and become more and more trustworthy as time goes on.,You could teach it to look for certain keywords.,It would be better for it to be able to learn and correct mistakes.,It still has a lot of room for improvement, so yes, a lot of feedback should be considered.
_instance_explain,frustration_why,Rather have human,Because some of the results were incorrect,even though it was easy to determine.,I think that it gets enough of them correct, even if it uses incorrect terms to do so.,The model was fairly accurate so I wouldn't really feel frustrated but would like to modify it to include more precise terms to look for.,Because it highlighted words that weren't beneficial in identifying the correct sport.,I think the model could do a much better job if it actually picked relevant keywords to focus on.,It seemed to generally be correct which would save me time despite the mistakes that need to be double checked.,I wouldn't feel frustrated, because it seems like a pretty accurate tool.  It's helpful.,It seemed like it was fairly accurate, it would just be a bit of a hassle to go back and find the errors.,The model was not nearly accurate enough or allowing enough transparency showing how decisions were being made.,I'm a perfectionist, so it would irritate me when things were incorrect.,There would be mistakes. Some of them could easily be avoided if it would recognize certain words.,I would not be too frustrated.,This model made more than a few mistakes at a level that would not be acceptable in terms of accuracy.,You have to give it time in the beginning to develop. Also it was correct most of the time already.,The model would definitely be of help. But it is not perfect which would lead to frustration. ,I think it was right more often than it was wrong.,It's not very good.,It would take longer to do my job because I would have to check for accuracy.,It was easier to skim over and see if it was right/wrong than having to read and make judgements on my own.,It would be a havoc tryng to sort out work related emal when the machine makes so many mistakes.,I think the database can be tweaked to do a better job so it would be frustrating a little bit even though it is doing a decent job with so many emails.,Because I honestly would not care.,Because of obvious errors that were made such as player positions, player names, team names and the obvious use of the words hockey and or baseball.,I wouldn't want the machine to make me look bad since it still makes mistakes.,The machine made too many mistakes for me to trust it to sort my boss' emails. If my boss found that there were a lot of mistakes I would be at blame, I wouldn't be able to say it's the machine's fault. If I'm going to be at risk of getting into trouble with my boss I want to be sure that I'm doing the best possible job I can.,It picked words that sometimes didn't really mean anything. Like it would pick stats or and.,I think that it is possible that there will be mistakes made.,I don't think it highlighted the best words in many cases so I think there would be a lot of room for error.
_instance_explain,learn_why,Need to compare,Because I was not able to provide reasons for incorrect answers,It should have after the corrections to its actions.,It would have probably learned from perhaps a few of the e-mails, but nevertheless this definitely means it learned.  This is dependent on which terms were successful or not.,Because I didn't see any improvement. It highlighted unhelpful words for a majority of the emails.,It's hard to say because it is unknown how the model chose the words it focused on.,I guess it would depend on the code of the model and whether it is written to accommodate the feedback properly.,This was difficult to know.  I'm not really sure.,I don't know for sure but if I used it again I would have a better idea but if it followed my advice it would probably learn something.,No feedback was provided on how the model was adjusting over time based on my feedback, but the yellow highlighted words seemed to remain off-topic.,There were only a few emails that were incorrect for it to learn from.,It was still missing some of the important words at the end.,I have no way to know for certain.,I do not know. ,I don't think that a large enough number to have a huge impact on it's learning process. It probably picked up a small amount.,I believe the model can learn. ,I think it was categorizing pretty well, except when there weren't too clear of keywords. But I think when you "correct" it it'll learn.,It didn't seem to get better, but I couldn't really tel.,It was still using the wrong words to search the emails.,It was told when it was right and when it was wrong. I feel like it learned from those.,It sitll made mistakes.,I don't really know how that was programmed. But I assume if it was incorrect, it would update its databases.,Again, I have no idea because the highlighted words were irrelevant more often than not. ,It seemed like accuracy kept going up as emails were completed.,I gave it careful feedback so it can learn from its mistakes.,I'm not sure right now. It's difficult to know if it learned until I see the next batch and see if it's mire accurate. If I was able to choose better keywords for the machine I would be more sure that it's learning, but I wasn't able to do that. The machine might believe that it's still using the correct keywords but it made a more stake elsewhere.,No, because it did not see my reviews until afterwards.,I think that it has the ability to see patterns and learn from the answers.,20 is not a big enough sample size. It needs thousands I think.
_instance_explain,accuracy_standard_why,Should be accurate unless there is no way to tell.,It depends on the context. If there is a name that is provided, the machine should be able to cross reference it and find that the person either played or was involved in either hockey or baseball. But then there are some that provide very little info and the machine might make a mistake, and in such a case, should return a result of unknown,Every program makes a few mistakes. Some of the texts are so small as to be hard to categorize.,This model cannot be 100% precise because a lot of e-mails can be vague and not include any terms that can differentiate between the two sports.,Because it's an easy task. There are very specific terms in each sport (runs, hits, ball, puck, goal...),I think the boss would want accurate results.,The model processes the data according to whatever standards it is written to have. If it makes any mistakes then it is because of human error and if a human is involved in something there is bound to be an error of some kind intentional or not.,These types of models cannot always get the right answer.  This is done by a machine and so there will be errors.,My boss would think I had made the mistake, not the program. There are some ambiguous terms that need to be read by a human to work out the difference.,Rarely are models able to be 100% accurate, so a few mistakes would not be critical.,Some of the emails did not have clear keywords to be able to tell either way.,I suppose no model is perfect. But there are definitely some keywords it didn't pick up that could have made it more accurate. ,I think a model has to be highly accurate since it is replacing human intuition.,Human beings make mistakes and no one can know everything. So, less than perfect is ok for a computer model also.,because it's necessary to learn. mistakes will be made until it learns which bits of language indicate which sport.,I understand that it will make mistakes just like a human would. ,Nothing is 100% accurate all of the time. As long as someone can quickly check the model, and help it "learn" by correcting it a few mistakes are acceptable.,If my boss asked me to do it, and ML lowers the accuracy, it makes me look extra bad since he thinks I used my own brain.,If the model makes mistakes, its mostly useless. I can sort through (and did) just as many emails and not make mistakes.,It should make a few mistakes since it is machine LEARNING. It's learning to do the task, it won't be perfect. ,it's ok to make some mistakes because it is a machine but there are quite a bit of mistakes. I think the machine's accuracy was very low.,Even human beings would make mistakes especially when the volume is like a thousand emails. After a few hundred a human would probably even make more and more mistakes. The trade-off between speed and accuracy becomes less important.,I don't know what the goal is. 100% accurate? 99%? 90%?,I think it is inevitable for the model to make an error every now and again, however I do feel like it made more errors than it should have based on what I was reading in the emails. As for using names of players to sort the emails that would be a bit out of my reach as I have no personal experience with hockey team player names.,The model can't be considered correct in its decision making if it's still consistently making mistakes. The goal should be to teach it to make the right decision all of the time.,There was one email that was impossible to decide if it was about baseball or hockey that only talked about an all-star game. So every once in a while the machine won't be able to figure out what sport the email is referring to. But for the most part there is enough the data for the machine to make an informed decision.,I think that if I am depending on it to do a job it should be done more thoroughly.,Because I would expect technology to be more accurate than if a human were to do it.,Maybe in the beginning, but with more data and refinement mistakes should be rare and not tolerated.
_instance_explain,overall,It is good at guessing,It was correct the majority of the time, but too many mistakes to be relied upon.,The model was good most of the time, getting the correct response.,It was good but not completely excellent because of the terms it was looking for.,Even though it got a majority of it correct, it almost seemed it was by accident and not of the model's doing. The model highlighted words that aren't really beneficial in identifying whether it's baseball or hockey.,The choices it made seemed random, so I really didn't feel I could trust the model.,I found it pleasant and useful.,It was easy to use.  I enjoyed using it and finding that it was usually correct.,It seemed pretty accurate but could be better.,Overall the model needs improved/expanded feedback options, more transparency into how decisions are being made and much more testing to improve accuracy.,It was interesting to see how it worked.,I think it's pretty good but needs some work.,I am ambivalent.,It did a pretty good job, but there is alot of room for improvement. It still needs more work.,fine. I would use it in real life. It would be worth your initial investment of time and effort.,It was fun. I enjoyed it. ,I think it did a pretty good job. The highlighted words made it pretty simple to see if the article was talking about baseball or hockey. Pretty interesting. ,It wasn't very good. I would not trust it at all.,I would not use it unless it gave me the opportunity to improve it quickly. It is easier to sort through emails in an email program using my own keywords.,I enjoyed it. I thought it makes the task easier to do and really helped out and made the job faster for me to do.,I do not like it. It is bad.,I think it would be okay on the scale of thousands of emails and if the error tolerance was fairly low.,Bored. Is this really a task so onerous and time-consuming that AI is needed? Seems like overkill to me.,I actually feel pretty good about using this model, it was accurate more times than not and if it had an option to sort three ways such as "definitely agree, definitely disagree, or I'm not sure" would be more than helpful.,I feel like it's a good idea that has great potential to learn and improve.,I enjoyed it. I would like to see if it can learn from the corrections I made. If I can get it to sort the emails more accurately I could see the model being a valuable piece of equipment for individuals.,It was very easy to use and for the most part it got the emails correctly sorted.,I thought it was good but I had a few that were questionable for answers and would not trust it completely.,It was ok, but needs refining and more data and tweaks in the programming to highlight more useful and pertinent words for each sport.
_instance_explain,perf_why,Would get better with practice,I have no reason to believe otherwise.,It would categorize the emails better if better word choices were shown to it in the reviews. In stead of simple agreements.,After learning from it's "mistakes" in the previous round, I expect the model to categorize more accurately.,Because I saw no improvement along the way.,I don't know what it learned because I wasn't able to provide detailed feedback.,If it is operating correctly, the results should improve.,I think it was pretty accurate to begin with.,I think it would be able to program the errors I caught to be flagged the opposite of the category it originally selected based on the key words.,The model does not seem to be adapting well enough to improve the accuracy of categorization at this time.,It would be better if I'd had the option to tell it when it should have looked at different keywords.,I didn't notice any improvement during the first 20.,Computers are consistent.,I don't know what would have changed in the model between now and the next set of emails, so I would say that it would stay the same.,I think it will exponentially improve as it goes.,I think the model will learn from its own mistakes. ,I think feedback helps to teach it.,It didn't seem to be learning anything.,I don't know the specifics about how the machine learns.,I wouldn't say it would do MUCH better but it would do better. It already had a good base to start with and every correction would be just fine tuning the process.,It is not a good model and made a lot of mistakes.,I think it would need many iterations to really feel that an incorrect word is incorrect specifically because it is another concept.,See above.,The model is had feedback with definitive information as to which sport the emails belonged with and it would have little choice than to learn from its errors and corrections.,As long as it can learn from its own mistakes, there's no reason it shouldn't steadily improve.,I think the machine has the ability to learn so I believe it can categorize the emails better after being corrected. The morevthe machine is used and adjusted the more accurate it can become.,I think that the more emails it does the better they will understand it.,I think that it has the ability to expand and do better.,It would be too soon to expect improvement after only 20 emails.
_instance_explain,trust_why,Rather have human,Because some of the results were incorrect,even though it was easy to determine.,It correctly selects enough emails to be useful.,The model was fairly accurate.,Because it got a few wrong and it highlighted words I didn't think were helpful.,It had a fair amount of success, but the keywords it chose to focus on didn't really make sense.,I would trust it to categorize the emails because it has been tested and performs in a predictable and reliable way.,It seemed to get the right answer most of the time.,I didn't feel like there were too many mistakes but there were some and that would make more work going back to find the errors.,The model did not provided accurate enough results to be used for anything other than testing.,I'm a perfectionist, so it would irritate me when things were incorrect.,I would have to double check it because it does make mistakes.,I am skeptical of computer accuracy.,The model made a less than acceptable number of mistakes.,It was correct the vast majority of the time and a small number out of thousands being a mistake probably won't hurt.,I could not trust it one hundred percent. It has mistakes. ,I would say it was right 80-90% of the time. If a few emails are misplaced into the wrong category it isn't a huge deal.,It's not very good.,It made mistakes that I wouldn't want to take credit for.,It mostly did a really good job of the sorting. I would rely on that to work for me. It might have gotten a few wrong, but overall it worked really well.,I would not trust it because it makes so many mistakes.   Bad track records.,It really depends also on what this project is for and the error tolerance that would be acceptable. More likely than not this model would do a decent job.,It's only 75-80% accurate.,The greater majority of the time I feel like the model correctly categorize the emails and it would at least make a good stab at sorting through a huge pile of email so as to move things along more quickly.,It's a good start but it makes too many errors for me to confidently stake my reputation on it.,The machine made too many mistakes. I can't put my job into the machine's hands unless it is going to be correct almost 100% of the time. I would rather take a lot of time manually sorting the emails but doing it correctly rather than having the job done quickly but incorrectly.,It did get the emails right for the most part but it rarely highlighted the most important words.,I noticed a few things wrong in the practice.,It was ok, but not as accurate as I would like or expect of a machine/AI.
_feature_explain,recommend_why,It is fairly reliable.,It may be useful, but not trustworthy,I might start with it.,It wasn't perfect but was better than nothing,I wouldn't use this model till some tweaks are done to it to make it more reliable,I think it's a good tool,null,I would use it only after I had taught it enough for me to feel confident that it would not make simple errors.,It's a crapshoot if it actually sorts correctly.,The model was often inaccurate ,Because the boss could point to a mistake that contained a strongly disambiguating word, like AHL or goal.,The model seems to do well with sorting based on the inputs.  Additionally using the model would save me time even if I had to spot check emails for accuracy.,I think it would be easier to manually sort the emails than trust this model.,It's not ready to do that yes. ,It was correct often enough that it would leave me a lot of time to do other things than read someone else's email for them.,It is not accurate enough to be up to my standards for a task, especially a work related task.,This would cut down on sorting emails and I would be more productive and efficient at other duties.,I am undecided. I think that it is getting there, but I feel that it would need further work before I would fully trust it with my boss's emails.,It is not reliable enough. I would want it to get a higher percentage right. ,It makes too many mistakes so I would not trust it to be very accurate.,Most of the emails can be checked with a quick validation.,Don't trust the model enough to use it.
,I feel that to use this model to help me sort my boss's emails.,I would definitely use this module going forward, as it was correct for the most part. As I've said, I would just feel a bit more comfortable looking over it myself after.,I think it could sort the bulk of the emails correctly and save me some time.,The model did a good job and predicted most of the e-mails correctly while highlighting a lot of key team names etc and players to help identify the emails.,I'm not sure I'd have to play with it more,There are a large amount of emails to sort, this helps narrow my job down and make it easier. ,I WOULD TRUST MY OWN JUDGEMENT MORE.,It would speed up the process
_feature_explain,how_decide,It chose keywords about each sport from the email and used that to come to a conclusion about what topic it was about.,It used words which could contain direct or indirect links to either game,It seemed to pick items with a slight margin of correctness. It seemed like it was getting help from a person sometimes.,It did pretty good and was only wrong a couple of times,It seemed to look for team names a lot or individual players names, it occasionally picked up on things in hockey or baseball pertaining to the particular sport such as pitching or ice,It used certain keywords to decide which.,The machine tried to use keywords from each email in order to judge the content of the email. ,It chose what it determined to be three key words and used the associations it had with those words to decide which sport they related to.,I honestly don't know. It was wrong a lot of the time, and picked random words sometimes.,The machine model recognized keywords mostly associated with both sports,Many words that I consider irrelevant were highlighted by the model, like "much".  Perhaps these occurred more frequently in one emails about one sport than another, but I didn't do a count. ,The model essentially identified the most pertinent words (assumed to be the most pertinent) and then assigned those words as baseball or hockey.  The accumulation of words helped to build an online bank or dictionary for each sports. The model then checks words against the bank of words previously inputted and makes the decision of what sport is being referenced.  It is an ongoing learning method for the model. ,I think the machine looked for a couple key words and used those to determine what the emails were about.  At this point, I think it is not nearly complete enough in its vocabulary to make accurate decisions.  A lot of key words were missing, including obvious ones like "goalie".  It seems to be a very simple model, and needs a lot more time and work to become even close to reliable.,The model was not bad at choosing which email was which, but many of the words it highlighted (e.g."greg" or "email") made no sense as words to find. I usually found maybe one or two overlap words that we both would select. ,It chose key words related specifically to each sport,The model seemed to look for key words including names of teams, players, and activities relevant to each particular sport, with varying degrees of success depending on the text of the email.,It tried to choose words that are generally associated with either sport. By using NHL for example it is able to determine the email is about hockey.Just like when I chose different baseball team names to help the model learn a broader range of information.,For the most part, the keywords selected by the model seemed to be random. It would occasionally select a word that had to do with hockey/baseball however, sometimes the words seemed to be random and had nothing at all to do with either sport. It did a fairly decent job of predicting which sport it was in the end.,I think it looked at key words that related to the game. ,The machine picked up on keyboards related to the sport on some emails and chose random irrelevant words on other emails.,The model was trained on a set of known terms and made a decision based on those terms.     Many of the terms were too general to make an effective decision.,The machine learning model highlighted works like baseball or hockey, but did not always make the right decision when those words were not in the email.,Basically, The machine learning model decides if it's hockey or baseball only by the input source.,It seemed to me that the machine was highlighting specific keywords that would associate with either sport. ,The machine tried to pick words more associated with playing the sport, but it doesn't seem to recognize some of the names of the teams or famous players of the sports. There were a few that listed the names of past players but there wasn't much other context for the model to decide. If famous players from each sport are entered into a database for the machine to search that might help it's efficiency.,It would pick players names and sometimes team's names. It also picked verbs (such as pitch etc.) to distinguish the sport. ,The model would take words from the email and decide if it was about hockey or baseball,Sometimes the model found highly relevant words, like hockey or baseball, or AHL(American Hockey League). Other times it seemed to pick random words that didn't have a clear connotation between hockey or baseball,  and I didn't understand how the model picked those words. The model was right most of the time, but it missed some. ,RECOGNIZING EACH SPORTS KEY WORDS SUCH AS BAT, BALL, PUCK,GOALIE,it took key words associated to the sport or players names into account mainly
_feature_explain,feedback_importance_why,It isn't completely accurate all of the time.,It needs to do better with secondary identification,If I had to use it I would want to correct it for later emails.,It has a lot of room for improvement,I would feel comfortable using it if I could teach it what words to look for until it got used to them,The tool isn't perfect so I like being able to improve it.,null,Absolutely, I don't feel that there's a point in having a program that I can't tweak to fit my needs further.,I think that's the only way it would ever be any good.,To give feedback in order to create upgrades and improvements to the model,Because there are keywords that should strengthen current performance, some of the currently used words are not even specific to sports (of any form).,A model like this will only work with continual feedback or else it would get stale quickly. For example, if player names is a good input (generally is unless the name is super common) it would need to be updated as new players join teams.,It needs to be improved and worked on.,It had several misses and even some hits were not grabbed because the terms it selected made sense. ,It is frustrating to see the same mistakes go by and not be able to correct them.,It seems very possible that with feedback the model could become a lot more accurate.,Just like we learn, I think this program will excel once it has been given all the words and slang of hockey and baseball.,I feel that it would need tweaks to get it ready and fully trustable with the results so far. Not being able to provide feedback would mean my boss would never have the ability to improve the results and in that case, it would not be worth as much since there could still be errors.,Feedback might help the model get more right. ,It seems like it needs much improvement because it was not very accurate and missed obvious keywords related to certain sports,Users need to eliminate common keywords (always, sometime), and ensure everything essential is included.     The requirement of three and only three terms limits the short-term success.,Feedback is very necessary in order to improve the effectiveness of the model.,It's more important to provide feedback.,Feedback is vital in a module like this, but it would simply be for constructive purposes. I honestly have nothing negative to say about this module.,I would want to be able to update it or make suggestions depending on what the data looked like, it's important to have that capability.,I would like to teach it to get better with experience and add words to it's vocabulary. ,It would be very important to provide feedback that way the system could get better,I would like to be able to feed it the most important words because over time I think that would drastically improve it's performance. ,IT DEFINITELY NEEDS IMPROVEMENT.,maybe narrow keywords
_feature_explain,frustration_why,It is fairly accurate. ,It is not perfect,I would worry about errors.,I wouldn't rely on it completely but I could use its help,It did pick up on things that didn't have anything to do with either sport so it can be a bit unreliable,I think it would be a convenient tool that would save money.,null,If the model worked, I can't see why I would feel frustrated by using it for that purpose.,It is wrong a lot of the time.,There were many keywords the model assigned to the wrong sport.,Because some of the mistakes contained words that are unilaterally associated with one sport, like puck.,I feel that the model is good. For each time it got the answer wrong, I could understand why based on the email.  ,I think it made too many mistakes for me to ever rely on it.,I believe it could be refined over time to select the right keywords. ,It would be helpful and seems that it would only have real problems with a few. I think that if the boss knew this was being used, he would accept the occasional mistake.,While some mistakes are understandable, there were other cases where some pretty obvious key words relating to one of the sports were missed by the model, which would be frustrating.,I feel this would catch 98% of what I am sorting though.And it will learn more as we use it.,Some of the words selected to determine the sports were fairly random and would not be of use.  Even if accuracy were fairly high, I would be concerned about the results and would feel the need to double-check them, which just counter-acts the whole point of using it.,Some of the games were wrong, and that would make me feel bad since it was my boss. ,It seemed like it chose random words that were not relevant to the sport category it selected,It works decently well, and a manual validation would probably be necessary for dozens of iterations.,I feel like the model did not always make the right decisions, but instead missed some obvious words.,I don't feel bad when my boss used this type of model's,I felt the module was correct most of the time, with only an error every so often. With that being said, I would not mind a hiccup every now and then. ,I think the model is good and seemed to do a good job. I can see where it would struggle with a few ambiguous cases.,I feel this way because the machine did seem to predict it right 90% of the time.,I might just because sometimes it doesn't pick words that are on topic,I understand that the model isn't perfect, and I will have to spot check to see that it's not making too many mistakes. ,TOO MANY MISTAKES. SHOULD BE AT LEAST 90% ACCURATE.,It mainly got it right each time 
_feature_explain,learn_why,I think it should be slightly more accurate with more feedback given to it and more knowledge to draw from.,I don't think it changed over time,It is possible my data helped it.,I corrected some of its errors,I think if it took my recommendations for words it needs to look for it could learn to categorize properly,I think my feedback was accurate and would help.,The words chosen by the machine were often random but I think my feedback provided more relevant keywords that would help. ,I think I did a good job of picking out words that were unambiguous; that is, words that would be expected only apply to emails about each particular sport.,It was still wrong at the end.,I didn't notice whether the model learned from the 20 emails or not.,I think the keywords I provided were useful, but I have not seen the model's performance since then.,I think that every word (whether repeated or unique) will give the model more data to do the job correctly so I think any new, accurate data will be a learning for the model.,I would need to see more emails to see if it improved.,I did see a few more terms selected that I had noted earlier. ,I would hope so. Sometimes it missed the actual words of hockey or baseball, but still managed to be right more often than not.,It seems that the model is capable of accepting feedback, which means it could improve in the future.,I was able to update new data for it to extract from.,I'm not sure that it learned as it still was selecting some random words that had nothing to do with either sport.,There were too many that were categorized wrong, so I don't think it learned. ,In some ways I think it will need much more feedback than 20 emails to learn to provide more accurate results.,Team names and league names (NHL, NL, AL, AHL) should be easy decision-makers,Can't tell if the model learned.,Yes, The model learned from the 20 emails.,I did notice as the emails progressed, the module began highlighting more obvious words, such as "Baseball, pitcher, goal, etc." ,I think that I selected some more obvious words associated with each sport so hopefully the model will learn as it goes along.,Somewhat, but as I went through it didn't seem to pick the keywords that I would have picked and instead picked gibberish words.,I think it probably did but I would have to do more of them to really see if it didn't,I kept hammering away at the obvious words, hockey baseball, league abbreviations. I would have to continue working with the model to see if it's actually learning from that. ,DIDNT NOTICE IMPROVEMENT TOWARDS END.,i tryed to choose different words that is always associated with the sport
_feature_explain,accuracy_standard_why,Some emails were ambiguous. ,Some of the emails do not have direct words for either game.,They add up quickly.,Nothing is perfect and some terms are used in more than just sports,I think its ok for the model to pick up on a few mistakes here and there till it learns completely what names and words to associate with which sport,No machine is perfect because of the wide variety of emails that exist.,null,If the emails are not going to be sorted accurately by the machine, then it makes no sense for me to run the program and risk getting chastised by my boss instead of just doing the work myself, no matter how tedious it is.,Sometimes it was hard for me to make a determination too so mistakes are natural.,The model can be improved.,Humans find this task fairly easy,so mistakes seem unacceptable.,I feel that the model will only be as good as the data put into it. If there are too many off topic or vague inputs then the model will suffer from the lack of good data coming in. ,I think that once the model makes some real obvious mistakes it calls into question how much it should be trusted and confidence is lost in the model.  Mistakes need to be extremely rare.,It's a learning process, mistakes are the opportunities to refine its parameters. ,Nothing, and no one is perfect, but the words for each are specific enough that it should be able to catch nearly all of them correctly.,There are some key terms that would be relevant to both sports, including generic terms like "players" and "score" or names of cities that have both baseball and hockey teams. There are also some emails that will be vaguely worded and difficult to determine.,Sometimes our email correspondence is in slang language, or not correctly spelled. Some team names may be abbreviated. Other times the email may not have any words relating to either sport.,If my boss is using this model to quickly sort through emails he needs accuracy.  If this model is going to take over someone else's job it needs to be more accurate than the person that it is replacing,Because it is my boss I would not want it to make mistakes. There were enough words to help it not make mistakes. ,Some emails are vague so I would expect a machine to make mistakes on those a human cannot even determine,Hockey and Baseball share a number of common characteristics (and thus keywords).     Thus it is impossible to perfectly train a model.,There were a few emails where it was really hard to tell if it was about baseball or hockey, so a few mistakes are acceptable.,It don't make mistakes.,I find it slightly acceptable for this module to not be 100% accurate all of the time; however, if it is being pressed and implemented, it should be correct more often than not.,My boss is depending on the task being done properly and they usually do not tolerate any mistakes being made. I don't think it's very acceptable.,I feel this way because technology is not perfect and I will never anticipate any computer getting anything 100% right. Computers are designed by humans, and there is no human that is 100% perfect as it is.,It is OK same way as it's OK when humans do it,There are so many emails of my bosses to go through, as long as the model is right most of the time it helps me out enough that I'll use it. ,JUST LIKE A PERSON WOULD BE- ITS NOT PERFECT. THERE MAY NOT BE KEY WORDS IN A SHORT TEXT.,it can interpret certain keywords such as game to mean either sport
_feature_explain,overall,It was helpful but there are still cases where it would be difficult for even a human to decide.,It is not ready for prime time, but it works for obvious words and clues, like NHL, MLB, baseball etc,It wasn't entirely worthless.,It worked a little bit,It did an ok job picking the sport the email was supposed to refer to but it did pick words that had nothing to do with either sport so I have no idea how it figured out which it was supposed to pick,I think it's a great tool and I'd definitely use it.,null,I actually had some fun reviewing the model's choices and trying to figure out why exactly it chose the sport that it did. The experience of reviewing each email was not tedious at all for me.,It wasn't a good model at all and was wrong quite a bit. Wrong frequently enough that I wouldn't trust it. ,Positive but there's room for improvement,Curious as to why it picked the key words that it did.,Great experience. I would use it and spot check. As I gained confidence in the model, I would ease off the spot checking and trust it to work on its own, especially if I was giving ongoing input.,It was an interesting model that seemed like it wasn't quite good enough yet.,It seemed like it was struggling to even apply a rational criteria to the terms it selected. ,It would save time and would prove useful.,The model worked fairly well, but I would need to have the ability to provide it with a lot of feedback and see if it could improve its accuracy before I would use it for any work related task.,I would and did feel comfortable using this program. The ability to teach it is exciting.,I feel that it does a pretty good job, I didn't have too many issues. I definitely found it quite interesting to see what words it decided upon.,It was frustrating. I thought the model would be more accurate than it was. ,I felt it was an ok experience. The model did well sorting certain emails but on others, it failed.,I thought it was good, although needs better training data.,I felt a little frustrated at the model's inaccuracy.,I Feel like this was the best email using model.,I felt I had a positive experience and the module did a pretty decent job of sorting the emails on its own.,I thought it did a good job and was easy to use.,I felt great! It was interesting to see what words the model chose vs. what I would chose. I think it did an overall decent job at classifying the e-mails.,I felt OK about it I just wish it would a use different words and that I could train it more,It was okay, on some of the emails it was frustrating because there weren't enough words that were relevant. For those I had to click a random word instead of the model only allowing me to enter two words. Also, for player's names I would've liked the ability to highlight two words and have it only count as one so the model considers both first and last name together. ,IT WAS OK BUT CERTAINLY NEEDS TO DO BETTER.,It was great it got it right most of the time
_feature_explain,perf_why,It has a better idea of what to look for and which terms it should recognize.,Its accuracy remained the same,I am just being hopeful.,Some of the key words were just common phrases,I think after I picked out keywords for it that it could do a much better job,I know specialized terms for both sports so I think my feedback was valuable.,The words I chose were much more relevant to either baseball or hockey making them more distinguishable. ,I would hope that the machine has sorted out its tendency to choose filler words or to skip over the obvious words like "hockey," "baseball," "MLB," and "NHL." That minor change alone would greatly increase the machine's accuracy.,Maybe slightly better if it able to "learn" from its mistakes.,I'm not sure,I think the keywords I provided were useful, but I have not seen the model's performance since then. Probably it learns gradually,so that new keywords have to battle against old ones.,Again, I simply believe that any ongoing feedback to a system like this will cause it to be incrementally better.,I think my word suggestions would improve it, but I cannot be sure.,I think that, with more feedback, it could be up the task and have fewer misses. ,I pointed out some words it did not seem to already know.,I think that the feedback gave the model many more relevant keywords to properly categorize, which would improve the accuracy.,It already caught 98% and after teaching more it will do better.,It might have the ability to get better if it is taking the input given and learning from it. ,It did a poor job of picking out words, and I think it would perform worse without feedback. ,It needs more examples of emails from each category to better sort them. Some of the emails in the batch were vague so these would not help it in sorting new emails.,I fed a number of good search terms, but the requirement to always have three forced inclusion of some ambiguous terms.,I doubt if the model learned how to correctly categorize the emails.,surely, It will categorize another set of emails very well.,The module did seem to improve as I went through the emails so while I would except it to do slightly better, I also may still anticipate a mistake here and there.,I think it would continue to incorporate the new data and update it's programming and it would improve.,I don't anticipate that it would improve at all. After seeing what it did I think it would act the same.,Because of the feedback I gave,If it takes into account the words that I entered, it will pick up on league names, team names, baseball and hockey right away, and this should give it a higher percentage of correct. ,JUST GUESSING. THINK IT NEEDS TO BE GIVEN MORE INFO.,better keywords for the sport
_feature_explain,trust_why,It is accurate most of the time.,There is too much room for error,It almost seemed more a distraction.,It wasn't perfect but was better than nothing,It didn't categorize a few right and it picked some words that have nothing to do with either sport,It's mostly accurate and the mistakes are understandable.,null,From the emails I reviewed, the program did a fairly good job at sorting the emails already. Hopefully my feedback will help it improve even more.,It made too many errors.,The model was often inaccurate,I expect accuracy to be much closer to human.,I feel the model did well based on the data provided.  I would still spot check emails until I had a good level of confidence that the model was mostly accurate.,It mislabeled some very obvious emails and completely lost my trust.,There were quite a few misses, so at this point I'd hold off until we highlighted more terms in common. ,I don't trust any machine completely, but it would save enough time that I think it would be a better use of company money than my pay, just to do something a machine can do.,The accuracy was decent but not at a level that would come close to matching my own ability to categorize the emails, so it wouldn't be up to my personal standards for the task.,After all the words associated with these two sports this program will greatly reduce my time in sorting.,While the model was fairly accurate, I would still be concerned with those that it did not do well on and would feel the need to check up on the results myself.,There were too many obvious mistakes. The model picked one when it clearly should have been the other. ,I felt it sorted many emails into the wrong category that quite obviously belonged to the other category based on keywords,The model seems to have a 60-70% accuracy rate.    When trained to look for the most important terms, I would expect it to get close to 95%,The model did not get enough of them correct for me to completely trust it to correctly categorize the emails.,I completely trust this moel.,Overall, the module was fairly accurate but I did notice a mistake every few emails. I would prefer double checking it myself after the module has made its choice.,I would still want to at least spot check because I think there are some subtle emails the model might miss.,Again, the model did predict most of them right so I would trust it.,I'd be a bit concerned you to the words that it chooses to pick to use to decide,Most of the times it got it right, maybe 75 or 80 percent of the time. If the job needed to be perfect I wouldn't trust it, but with the number of emails to go through, it's good enough. ,SHOULD BE MORE ACCURATE THAN IT WAS.,because it was right the majority of the time
_instance,recommend_why,It was accurate enough for me to want to use it.,It's a tedious task and I'm sure my time cane be better spent doing other tasks. The machine model should be able to sort them faster than I would be able to. ,it would save time overall,It would help to save time and I could go back and skim through them to get my work done quicker.,It would be easier than doing it manually. I would still double check it's accuracy though.,Yes, I feel like it would get through more than I could in the same amount of time, and if I was rushing through it all, I'd make mistakes too. So I think the benefits outweigh the problems.,If I could set parameters about confidence, yeah. It's way too big of a task to do by hand.,It seemed to do a good job,I would use this to help me because I think it would go through a lot of emails in a shorter amount of time. However I would not completely rely on it because of the mistakes that I found it had made. So , I would use it as a tool , but not as the only method. ,Saves time and is mostly accurate.,I would use it with caution, to make the process slightly faster, since I can skim for trigger words, or outright search for trigger words with word find.,I probably would and the would scan and double check the accuracy.,This method would be faster than sorting thousands of emails manually.,The model as it is makes too many mistakes and would not save me time getting this project done.,As I said earlier, there is no telling how many mistakes it would make. ,I don't know if it would be that useful.,It would b e much quicker than reading each e-mail. Also, I would trust the system's overall accuracy. ,I would absolutely use the model. With the ability to give it feedback it would only get more accurate over time requiring less direct interaction from me. This is exactly the sort of task a machine can excel at.,The model would help save time and allow me to do other things with my time.,I do not believe I could rely on it to the degree I would need to in order for it to provide the results my boss would expect.,not sure if there are better methods or not,It was right the majority of the time.,Difficult to expound on the above; i would use the model because i believe it to be a useful tool that would improve the efficiency and accuracy of my work.,Because not only does the model seem to be overwhelmingly correct, but it would also save me lots of time.,Even though it made mistakes it would probably speed up the process. After the emails are sorted into two groups I would be able to skim them for mistakes.,I couldn't feel good about using it just yet, not until it can do it with 100% accuracy.,With a few tweaks, it might work better.  A more robust keyword system might be a start, or the ability to associate player names with a specific team and league,I feel that it would be a time saver. ,The model got the majority of the email's subject correct, so I believe it would help me sort the emails quicker. I would only be required to do a quick review of the model's decisions instead of having to do all the work myself.,I believe this tool can do the job with some human supervision. I understand that it isn't perfect but still much more efficient than doing it manually. It'll save me time.
_instance,how_decide,It used keywords that made its predictions pretty accurate.,The model correctly categorized most emails, although there were a few that were not correct, and on those I was puzzled how it incorrectly sorted them. The couple of emails that weren't categorized properly actually had the word "baseball" or "hockey" in the email and the machine model categorized them as the opposite. The best I can tell is the machine model used team names to correctly sort emails. I think it also has a bank of key words for baseball such as " home run, pitcher, innings, etc" and a bank of key words for hockey. ,It seemed to look for specific keywords. Most of the time it was right and a couple times it wasnt. ,I think it would take into account the grammar used by the emails and evaluate the content to make their decision.,It picked up on keywords that are common in the different sports. It was accurate too! I think it only messed up twice, and it was a little more difficult to decipher.,It seemed to do a fairly decent job distinguishing baseball emails, especially if there were keywords like baseball, pitcher, or base. I think it did an OK job making determinations when there were baseball team names involved too. It seemed to have a really hard time making determinations when there were abbreviations like AHL/NHL, player/coach names (like Jagr), or technical sports talk though. It hadn't seemed to learn many of those things and seemed to strongly default to Baseball when it wasn't sure about how to categorize something.,It searched for key words (the obvious -- hockey, baseball -- but also nonobvious like ball or goalie or plus/minus or other stats) and matched those to presorted categories hockey and baseball.,It based it's decision based on the  wording of the email,I think the machine learning model did a decent job at deciding whether emails were about hockey or baseball but it does need some improvement. I think it most likely decided by scanning for key words in the emails. I think it can be improved by possibly adding to the key words it looks for. I found a few guesses by the machine that were wrong and only one that I was unsure about ( which asked for all star tickets . I know that there is a MLB all star game and I believe NHL has one too. So this could've been about either sport.).,Most of the emails contained key words associated with each sport. Such as puck, throw, and even team names. Just looking for those words would get a correct answer a majority of the time. Then, there were a few emails that had few details, like just names. Those require in depth knowledge of who is playing or even some educated detective work. Like if an email was from a .ca, then maybe the model would decide it's about hockey cause of that.,I think the machine was probably programmed to find trigger words, such as MLB, NHL, baseball, hockey, pitch, goal, etc. Possibly even to find names associated with baseball and/or hockey players.,I'm really not not sure. There were a few obvious errors where the name of the sport was listed in the message but the model still picked the wrong sport,,I think the machine learning model looked for keywords. For example hockey emails would include words like goalie, AHL, NHL, puck. Baseball emails would include pitch, MLB, homerun. The model might have also looked for cities where teams were located. A mention of Vancouver would mean a hockey email. While a mention of San Francisco would mean a baseball email.,It checked to see if the words hockey or baseball were used.  It also used statistic abbreviations like HR and league abbreviations like MLB, AHL, NHL, team names and player names.  For baseball it used pitcher to get the correct answer but the machine did not respond correctly to the word goal or goalie often putting those emails in the baseball category.,I feel like the model typically scanned the email for the words "hockey" or "baseball". Sometimes it got it right, sometimes not. ,I don't know how it decided but it seemed to be correct most of the time.,I'm going assume that it was able to analyze keywords within the e-mail that are associated with either baseball or hockey. I'm not sure if they keywords were included in any type of database. I think when the machine understands the word association, it's easy to determine where the emails fit in.,I think it used keywords and player names to determine subject matter.  Although it wasn't wholly accurate in this endeavor and managed to miss emails with whole words like "hockey" to miscategorize them.  It was much more accurate when looking at player names and sport specific nomenclature/vernacular.  ,The model relied on searching for key words specific to each sport. These could include team names, game descriptions, and player names.,It looked for player names, the words "hockey" or "baseball," and terms such as "MLB," "NHL," and "AHL. Even though the one email was really about a board game, it was a board game about baseball that was being sought. So the email was not really about current or historical aspects of the game or players, but a way to play a board game version of baseball. So, that helped me see that the algorithm looked for the word "baseball.",keywords that are targeted toward hockey or baseball,Based on the content of the email and hockey and baseball terms,I believe the model used keywords and phrases (AHL, NHL, MLB, etc.) to determine the nature of the email. It wasn't always correct and probably used additional factors, but i believe this to be the main MO of the model. ,I'm not entirely sure because not only did the model make very few mistakes, but also the few mistakes it did make were with respect to emails that did not have very much, if any, identifying information. If I had to guess, the model is well-trained with respect to standard features of baseball/hockey, but perhaps it defers to baseball if there is not enough information?,It looked for keywords in the emails that could be interpreted as being about one sport or the other.,Looked for keywords that were associated with one term or another.,Looked for certain keywords in the subject line and body of the email, such as mlb, nhl, baseball, hockey, goalie, plus/minus, pitcher, catcher, batting, goal, etc.  It then associated those keywords to the correct sport.  However, when these keywords did not exist, such as emails with only player names, it had a harder time making the distinction.,The machine looked for keywords such as baseball, ball, pitcher etc. I noticed that when specific players or teams were mentioned is when the machine got the sport wrong.,It seemed to scan the text of the email and search for certain key terms, most obviously 'hockey' and 'baseball'. It also seemed to search for terms like 'nhl' and 'mbl'. It seemed to have trouble when only proper nouns (the player's name) were mentioned in the email without any other keywords. Although I noticed one email the word 'puck' was the only key term included, and the model did not pick up on that.,I think it searched the whole email for keywords that were relevant to the specific sport. Things like the actual name of the sport(hockey or baseball), player's names, sport specific lingo(pitchers, goalies, runs, etc), acronyms(NHL, MLB), and team names.
_instance,feedback_importance_why,It did make enough errors that it could use some correction.,Yes, it would be important to provide feedback to help make the machine model more accurate. ,everything needs improvement. Thats always important,It would help to perfect the model and to make it more accurate.,It could learn more things if it was given the proper feedback on what it did wrong.,If it never receives feedback it has no way of improving or actually living up to the learning part of machine learning. I feel like it should maybe have a confidence rating level and someone could manually review those low confidence emails, provide feedback to the model, and help it improve over time. ,It was wrong at times and needs to be fixed.,I really don't think it's necessary,I would like to be able to provide feedback so that it could hopefully be improved and eventually become much more accurate. ,It didn't categorize all the emails correctly. Giving it some feedback as to what to look out for could help it.,Because it did get some of the emails wrong, I think there should always be room left for improvement, which means providing important feedback about what it got wrong and how often it happened.,The model still needs to be improved.,I would want to opportunity to add additional keyword rules that would help the program with its accuracy.,I need the machine to learn so that it can become more reliable.  Once it is reliable I would feel like I could actually use it effectively,Maybe with more feedback, it would be able to learn to distinguish more proficiently.  ,It might improve the functionality of the model.,It's not perfect. So, having the ability to  help it improve would be a good thing. ,While the machine was already fairly accurate, we are talking about sorting emails written by people using a living language.  Slang, new memes, new player names, and iconic events will change the way people write their emails and the content in those emails.  The model would absoultely need to be dynamic enough to receive further training in order to keep up with these changes.,I could adapt the model to use it for other tasks as well.,I would not really want to actively participate in the evolution of the product. I would expect it to work out of the box.,its always helpful to provide feedback,I would want to increase the accuracy.,The very name of the model is my answer. I believe it needs to be constantly learning in order to improve. The more it learns and the more accurate information it has access to the better it can function, imo.,Because I'm not sure the feedback I would have would be particularly useful. The few times that I disagreed with the model, it was not obvious to me that the model was wrong for any reason other than there not being enough useful information for it to do its job.,I would want the model to eventually do it's job without my having to double check the work. The only way for that to happen is if I can help to teach the model the correct answers.,If feedback helped improve it, then it could get to  100% accuracy.,Without feedback, there can be no improvement.,I think with the help of feedback it would improve with time.,It would be important to provide feedback so the model can augment it's knowledge base and incorporate more information into the way it makes decisions. The model can be improved by adding to it's knowledge base, maybe adding lists of famous player names and other sport specific terms like 'puck' 'ball' etc.,I want it to improve so as it learn more from the mistakes it will be more accurate. Ultimately it's up to me to improve this program. It's about what I put in to help this program evolve.
_instance,frustration_why,It was useful so it would not be frustrating.,It seemed like the model had a fairly high accuracy rate when sorting emails. ,I would feel like i would still have to double check everything,I think that the majority of the time it is correct and it would save time if used.,I would think it'd be about 98% accurate, so I'd be happy with that.,I'd be frustrated at first, but no more than going through a thousand emails manually. I feel like the benefit of machine learning is that it can learn and improve.,It is helpful, but it got some wrong. Still, it would be much faster than manually sorting every one.,I think it did an expecellent job,I don't think it would be frustrating using this model. I think it would be helpful , minus the mistakes. If there was a high volume of emails to go through and this could sort them quickly with not many mistakes , it would be helpful. ,It got the majority of them correct. Mostly working as intended, so there would be little frustation on my end.,If it was absolutely imperative that each email be appropriately labeled, then I think it would be somewhat frustrating, but nothing too bad, because I have other tools, such as word find, at my disposal, which would make verification faster for me, even if the bot got it wrong.,It would depend on the subject nature of the emails.,I felt that the model did a good job and with some tweaking and the addition of more keywords it will do a better job of sorting. I feel access to a database of hockey player names vs. baseball player names would help sort the emails better.,Words like goalie and goal should clearly identify an email as a hockey email.,While I believe it is acceptable for the system to make mistakes, it would be frustrating letting it do my work. I would feel that I need to check it's work, which defeats the purpose. ,I think it would make the task go faster.,I think it's accuracy was pretty good. I don't expect it to be perfect, but it was very accurate.,The model seemed to be mostly accurate.  The number of errors that were produced in my 20 samples were far outnumbered by the number of emails that were correctly sorted. ,The model simplifies the task and that is a good thing.,I think it would let too many non-relevant emails get through.,I trust this model enough,It was right the majority of the time.,It was right more than it was wrong; i believe it would be a useful tool in helping me accomplish my task more efficiently. ,Because the model did extremely well on categorizing the emails that, and the emails that it made mistakes on did not look particularly important or anything like that.,This model seemed to make some mistakes. It would be frustrating to have to check up on the models work and correct it.,It can almost do it perfectly but not quite.  Boss would be mad if I made mistakes with his email.,Because I would have to go back and manually check every email anyway,I think for the most part it did well and it would save me time.,I would not be frustrated because the model was correct the majority of the time. If I used the model, it would only require me to briefly review the decisions, rather than having to read the email and make decisions on my own. It would definitely save time.,I understand the limitation of the program and adjusted my expectation accordingly. It worked well enough that I think it would help me with the job despite the flaws and errors.
_instance,learn_why,I don't know if the feedback was specific enough for it to learn from.,It's hard to say because although I would tell the machine if it were right or wrong, I wasn't giving exact feedback as to why. For example, highlighting or inputting key words that belong in baseball or key words that belong in hockey. So it's hard to know if the machine learned why it was wrong. ,It made very few errors so Im assuming it learned along the way,I think that the model learned from its mistakes and would correct itself based off of my feedback.,I am not sure if it learned anything new.,Yes, I feel like it's going to use those emails I told it were wrong, and apply the keywords or key components it finds in them against other emails it receives, and slowly improve perform performance over time. ,It got worse toward the middle/end, but it probably did learn.,Seemed to catch on as it went through the emails,I am not sure if it learned from the 20 emails , but I would hope that it was able to learn and improve .It would be a very useful tool if it could become more accurate. ,Probably learned more from all the samples it categorized. "Feedback" good and bad should help it do better in the long term.,I would hope that it did, but I have no way of knowing for sure. If it used some of my input, I think it would have learned to some degree.,It probably adjusted the way it sorts based on the corrections.,I'm not sure if the model will have access to more names or keywords that will help improve its accuracy.,I think the machine would "learn" by scanning the emails it got wrong and putting the words in the correct category so that once encountered again it would categorize it correctly.,I wouldn't be able to say whether it learned or not unless I used the model again and compared results. ,It may or may not have learned anything.,I think there were some ambiguous statements that were hard determine. However, there was no way to explain that. So,I don't know if my input would have  been that helpful.,The model will learn more from its mistakes than from its successes... in that respect there were some incorrect categorizations that would be "easy" to correct... leading to higher accuracy in the future.,The task was simple so I don't think it was too challenging.,I have no experience to prove it either way as it supposedly did not apply my feedback until the end. I would not know if it "learned" anything from my feedback unless I used it over and over again through many trials.,i feel that there is at least some sort of feedback mechanism for adjustments,It was pretty accurate,If it is a true machine learning model then it should learn from every piece of information it gets, no matter how big or small.,I don't know! I would need to see it work through some more emails.,It was informed of the incorrect choices it made and learned to recognize more vocabulary.,It needs more practice so it won't make mistakes.,It's not enough just to tell it the correct answer.  You need to be able to explain to it why it made the wrong choice.,I think the more feedback it gets the more it would learn.,From the emails that the model miscategorized, which mostly included only player names and no other indicators of the sport, I do not see how the model could have learned anything without specific input.,I couldn't say for sure since I just provided my feedback and haven't seen the updated result.
_instance,accuracy_standard_why,There aren't always keywords available to make predictions,There were a couple of emails that were hard to determine the sport because of the lack of key words or team names. In those cases it would be acceptable for the model to make a mistake. ,well because its programmed by humans its not going to be perfect,I think that it is still learning to correct it's behavior and to make perfect decisions based off information given.,It is a machine, and it's bound to make mistakes. A person without a lot of sports knowledge would have the same problem.,With the volume of emails my boss receives, even a human going through them quickly is going to make mistakes. I think over time as the model learns more and receives corrections and additional dictionary items, it would improve. ,It made too many mistakes for ones that were obvious. The all-star one was ambiguous, but most were not. And it got some of them wrong.,Even humans make mistakes,I think that a few mistakes don't make a huge difference , and when my boss is reading them he will be able to see that. But , no mistakes would obviously be better than a few. ,For a model that is starting out, it's expected that mistakes creep up. A more mature one, less so.,Whether or not it's acceptable for the model to make mistakes is entirely dependent on how important it is for the boss to have the emails appropriately sorted. So in this case, that would be their decision to make.,The model is still learning and improving.,There are some scenarios such a mention of All-Star game tickets that do not provide any other context clues. Both hockey and baseball have All-Star games, so the machine would only have a 50/50 chance of sorting that email into the correct sport.,If the machine is not perfect that means I have to check every email anyway, so there is no real benefit.,Sometimes the way we words stuff is confusing to A.I. systems. We can confuse fellow humans with certain phrases, so it is understandable that the same occurs with machines. ,It is a machine.,I think there are some ambiguities for example one email spoke about an All-Star Game. This term can apply to both baseball and hockey.,This is an algorithm evaluating text written by humans who are using a living language.  The machine has set rules that have to be adjusted through experience and doesn't have the advantage of a human mind to better interpret more subjective phrasing, typos, new slang, etc.,Hockey and baseball are easily distinguishable from each other. Specific words, names, and descriptions do not tend to overlap between sports.,If I was relying on the model to function in a commercial environment, I would demand close to 100 percent accuracy. This precision would be really high for serious sorting, such as for medical doctors or threat searches.,Not all models, in fact very few, are perfect, just like with humans there is always room for error,Nothing is perfect all the time.,No model can be expected to perform flawlessly, especially given the fact that human beings would make their own share of mistakes. Not even machines are infallible. I think there is an acceptable error rate in almost anything.,Given that the model is intended to expedite an otherwise tedious process, a few mistakes are acceptable if the model is faster than doing the work manually. In addition to this, I don't think the decisions it's making here are particularly weighty, so I'm not sure why it would matter if it gets a few things wrong.,If the model makes mistakes it will be blamed on me. If I have to double check the models work it won't be very useful.,By making mistakes, it puts hockey emails in with baseball and vice versa.  It's not supposed to do that.,If I was to submit this to my boss, I would want it to be flawless,It is not human and does not know everything about the sports.,It is acceptable for the model to make a few mistakes because it is lacking the knowledge of nuances and slang that may be contained in the emails that are necessary to determine the subject. It is understandable that it cannot distinguish between every player's name to deciding whether the email is about hockey or baseball. The model sorts the majority of the emails correctly and a few mistakes are ok.,Machine learning even though sophisticated as the technology develop is still a work in progress. It's nature that some margin of error can be expected. It still worked reasonably well in my opinion and can still save time compare to doing it manually.
_instance,overall,I think it was useful and only needs a few adjustments.,I felt it would be helpful in the tedious task. As long as I could give feedback to the model to help improve it, I would feel better about it taking on the task. ,It was fine. seemed to make very few errors,I thought that it would have a lot of use and would help to speed up work time. It would be correct the majority of the time and by giving it feedback it would be more accurate.,I Thought it was fine. I didn't have any issues using it and think it worked pretty well.,Overall I feel like it was a positive experience. The model was right more than it was wrong and had a method built in to provide feedback to it for improvement. I thought the feedback was quick and easy to provide.,Mostly good, but sometimes it missed obvious things.,It was very interesting.,I thought the experience of using this model was good. I think it was probably about 70-75% accurate .,Positive. Seems to mostly work, which made my end easy.,I thought it was helpful, but not 100% reliable. And even though it wasn't entirely reliable, it still made the process of going through the emails a little faster.,I enjoyed it. It was easy to tell the correctness of the messages and the categorization.,I think the model works well, better than I had expected before I started the experiment.,I thought it was a simple task which is why I did not understand the machine's errors.,It got most right, but it could still be improved. ,It was pretty good.,I liked it. It was easy to use and proved to be very effective.,I actually found it pretty fun.  I'm not sure I entirely understand how it made decisions... as it didn't seem to be consistent in decisions.  I did, however, find it a lot of fun to try to figure out how it was making those decisions and how I would do it differently.,It was very simple to evaluate and the results were good.,Interested in how things like this can be applied to other topics or subjects.,most positive and accurate,It was overall fine, could be better but majority correct.,I enjoyed it. It was right more than it was wrong and i think feedback would help greatly in improving its performance. ,Pretty good! I was expecting to have to do way more correcting of the model than I actually did.,I liked it. Once it improves it would be very useful in speeding up sorting tasks. ,Not too bad.  I thought it did fairly well.,It has potential, but still needs tweaking in order to perfect it.,Overall all I like the concept and would use it to help save time.,I enjoyed using the model and I believe if I had a lot of emails to sort, it would make the task much quicker and less taxing on myself. It would be useful to only have to review the model's decisions rather than have to read all the emails manually.,I felt pretty good. The result was actually better than my initial prediction. It had exceeded my expectation.
_instance,perf_why,Because it didn't improve.,Again, I didn't get to input the information (words) that the machine should have recognized as belonging to a particular sport, so I have to wonder what it "learned" about the emails it mis-categorized. ,It already did a good job. So I expect an even better job. ,I think that with my corrections that it would do better on another round of emails.,If it found out more keywords to search for, it would probably make fewer mistakes the next time it was used.,I don't think helping it with 20 emails is going to immediately make it foolproof and perfect, but I feel like any type of feedback the system receives is going to improve the algorithms and improve overall performance of the system. So I think it would do better, but it might only be fractionally better.,If it's machine learning, I would hope it would learn after receiving feedback.,I think the more it does the easier it will be.,I think that the model would possibly take my feedback from the first twenty emails and use it to improve. So, I would hope that the model would do better if given another set of emails , because of my feedback on what it got wrong ( and right). ,Depending on what it learned, it should go slightly better than the previous run.,I have no way of knowing how intricate the system is, so with consistent help from users, I would think that it would slowly start to get better and better with more input.,I'd say the feedback would improve the overall accuracy of the model during the next run.,The model will hopefully look at different keywords or more keywords if it encounters a similar email to one that it had previously not categorized correctly.,I think it takes more than just a few mistakes to teach the machine, it would need many and repetition,I think it would take more feedback than 20 emails to make a large improvement. ,I think it needs more testing.,I think it would improve. However, I think there wouldn't be that much of an improvement for emails that don't have terms that are typical to either sport.,There weren't many mistakes to begin with... and the emails are going to be full of vague emails that can and will trip the model up.,The model seems to be able to handle this type of task well.,Because I have no experiential data to go on.,it seems pretty straightforward,Just a feeling.,I think the model would have learned something, even if it is a minute something, and used that to improve its performance. It may take longer , but i like to think the increase in feedback data would help improve it  with an answer or two.,Because I'm not sure the model was missing any obvious clues. Like I mentioned several times, the emails it seems to have issues with were emails that did not much useful identifying information.,It probably learned some from it's mistakes, but it would take a lot more than 20 emails for a significant improvement.,Because 20 just aren't enough.  It still made mistakes.,The feedback is minimal.  It may not understand the reason why I'm telling it that it is wrong.,I feel like it would keep improving.,I do not see how the model would have improved from the feedback I provided. The model does not take into account why the subjects were wrong because I did not specify anything.,I choose to believe in the program's ability to learn. I had a good experience with the program and it left a good first impression. I felt optimistic about its prospect to improve.
_instance,trust_why,It seemed to work correctly 90% of the time.,I never fully trust machines to do anything. If there was limited information and slang sport terms, the machine model may not be able to determine the sport whereas a human who was well versed in both sports could. For the most past, however, the machine did its job. ,I think it would make it that I can save time by quickly double checking em,I think it got a high percentage of categorizations correct.,It was accurate enough to sort them, in my opinion.,I feel it would get them correct more often than not and would improve its accuracy over time.,Again, it was mostly right, but still got some wrong. So I'd set parameters about confidence and such.,It done a great job only missed a couple,I feel this way because for the most part it sorted correctly. But , I don't strongly agree that I would trust it, because there were some mistakes that I found .,Most were categorized correcly. So can put quite a bit of faith in it.,I would trust it mostly, but not entirely, because it did make some mistakes.,The model was correct the majority of the time.,I feel the model worked about 70% of the time and small fixes to the program could help it become even more accurate. ,There were too many errors, I would want to scan them all myself even if the machine went first.  I'm just not sure this is a time saver,It made more than 3 mistakes in the batch of 20 emails, I could only imagine the amount it would make in a larger batch. ,It makes mistakes.,I think the accuracy is pretty good. If it can learn, then I would say that it would probably be able to improve. ,I think this would be an excellent way to accelerate the job and possibly get a first rough sort through.  The model could be adjusted after the first sort and run again.  Multiple passes would still be faster than going through the entire sort manually.,The model did a good job with the analysis in this case.,The board game email is an example. I don't think my boss would really want to see that type of email even though it was related to baseball.,they are similar and a couple errors is no big deal,It was right the majority of the time.,It did get some obvious emails wrong and the error rate is a bit higher than i'd like, but overall i think i would trust it to get better over time and be a helpful and useful tool for me.,Because the model seems to be overwhelmingly correct.,It made too many mistakes for me to present the work to my boss without checking it over first.,It's not perfected yet and makes mistakes.  Boss wouldn't like it.,Too many inaccuracies ,I would not fully trust it I would have to check its work but I still feel like it would save me time.,I would trust it in a broad sense but it does also make a few mistakes, so I wouldn't trust it completely without my own review and input.,I trust it enough to do a good enough job that I can pick up from where it left off. Make the correction myself and still save a lot of time.
_explain,recommend_why,It would be a good tool. It does a good job for what it is, just not 100%. It is a tool, not a human replacement.,I think it could be helpful to do a first pass on sorting the emails.,I can see it being more useful than not. It'd likely help my productivity more than hinder it.,There are too many errors. ,It would help me and save time.,I would. It would make mistakes at the beginning but get better as it goes. ,because it isn't perfect I might miss one. ,I would have to review each of them, so it wouldn't be much of a time-saver. I would only use it if I had no alternatives.,I think it would save time, so i would use it. i'd just have to quickly double check the model's work afterwards.,It would not be helpful and I would get in trouble for doing a bad job sorting.,I don't trust it to sort between sports so I wouldn't trust it to do much else.,I will use this if it is available at my job to use,I could do a better job myself -- too many errors,The model identifies nonsense words in trying to decipher whether or not an email is about baseball or hockey.  It can't be trusted to be accurate.,I feel like it's randomly selecting words and then making a random decision from those selected words. Two random's don't make a right. I doubt it would select correct words to efficiently sort the emails. ,The model made some errors but overall it was accurate.,In its current form, this model would make enough mistakes to get me in trouble with my boss.,It was not useful for actually doing what it was supposed to,No, I would not. I would rather go through them myself. I would honestly be so paranoid about the results being wrong. I would rather do them myself.,IT would be faster than me sorting through them.,Because it does not perform as well as I think that it should. If it could be tweaked to notice some obvious terms then I may reconsider but I would not use it as it is right now.,Same answer as above. I'm not comfortable trusting it since it has proven to be accurate.,I didn't agree with what the model said whether something was baseball or hockey.,Th level =f accuracy is not high enough.,I would have to go back and read all the emails anyway because the model got too many wrong.,It is more efficient than having to read each email.,I would try to use whatever tools would make the job more efficient.,I would only want to use it if I could refine it a little bit. The highlighted words should be scored. Emails that don't achieve a certain score should be separated for manual sorting. Refine the model until the amount of separated emails is manageable.,It doesn't pick up key words that would help.,It is not accurate enough.
_explain,how_decide,It looked for key words that might help determine which it was. This could include something related to the sport or a particular word such as "cup" that might refer to the Stanley cup. The words highlighted were considered relevant to the particular subject.,It looks for specific words that are related to either topic and categorizes the email based on those words,Sometimes the words used by the machine learning model made sense and were words related to the two different sports. Sometimes, the words seemed random and when that was the case, the accuracy was a crapshoot. ,Used key words. ,It picked out key words that it thought were related to one of the sports or the other.,It associated different words with each sport. It was able to pick out team names. It knew the words 'baseball' and 'hockey'. If it had 'cup' in it, it was most likely baseball. ,I think it figured it out based on different key words in the paragraph. ,It wasn't always clear because the highlighted words were not always good clues to identify the sport. Usually I could identify the sport best from non-highlighted words, such as a team name, or the league acronym, or a mention of the sport itself. Nonetheless, the model was pretty accurate. Maybe the highlighted words weren't the only ones the model relied on to make a prediction. Maybe the model scans the whole email and picks up key words, like team names. I am really not sure why the model chose the highlighted words as most important.,I'll be honest, I'm not sure. many of the highlighted words were not relevant to either sport. Sometimes they used team names and other times team names were ignored. Cities were often used, but the one where they used Montreal for baseball (and the AI was right it was baseball) made little sense as Montreal is known more for their hockey team than their former baseball team. Also, NHL was used as a keyword, but the AI chose baseball.

So all of these inconsistencies, along with poorly chosen keywords, make it hard to determine the method used ,It did not do too well and missed many of the emails, some were not for either baseball or hockey.  Imagine how many mistakes it would make if it were thousands of emails?,It tried to pick out keywords that would pertain to a certain sport. "pitch" or "single" would pertain only to baseball so it classified it as such. "goal" would be exclusive to hockey.,by sorting them into baseball/or hockey.,From what I can tell, the model used "key" words -- or what it interpreted to be key words. Model seemed to have the hardest time w/hockey -- either choosing baseball when it was hockey . . . missing key words when it was hockey or saying it was hockey when it clearly wasn't,It would pick out well-known sports words like, ball, NHL, stick, Braves, etc...  that would let it know if the email was talking about baseball or hockey.,First of all, most of the time the yellow words were conjunctions or words that were not sport specific. It looked like it was just selecting words at random and not looking for specific words. You could program it to search for sport specific words such as: baseball, pitch, catcher, glove, bat etc for baseball related emails. And search for: Hockey, cup, skate, ice, puck, stick, etc for baseball related emails,I'm not exactly sure but the machine learning model did highlight some baseball and hockey terms to determine what the email is about. ,The model chose words that sometimes seemed related to relevant sports concepts, but sometimes were pretty random. A number of times it chose to focus on random words even when obvious sports words were in the email, which I found very confusing.,It used certain keywords that it found more relevant to either hockey or baseball.,Basically it had three highlighted words that it thought was vital. It seemed to be flawed at times though.,There were specific highlighted words but none were REALLY that accurate.,It seems that sometimes it really understands the keywords but other times it really misses them. The main things it looks at are some action words and some general words. I think the general words is what seems to throw it off and not be as accurate.,Well, it appeared to look for certain keywords associated with the different sports. Unfortunately, it selected a lot of completely irrelevant keywords, such as , much, got, and really. It also assumed "cup" was always hockey, when in one case it was legitimately talking about a cup. So, in theory, it's a great idea, but it's execution needs a little help.,I noticed that it was able to distinguish baseball where it highlighted baseball, pitch, etc. The hockey one seems to have a lot of players name.,The machine looked for certain words, such as pitch, which would apply to baseball and goal ,which would apply to hockey. The method did not seem to be foolproof as several of the emails were obviously labeled wrong,I have no idea.,The machine learning model decided whether emails were about baseball or hockey by choosing words related to the specific sport.,The model seemed to scan for keywords within the text.  I would assume there is some sort of consensus on how many of the keywords there needs to be related to Hockey or Baseball.  ,The model made its decision based on keywords deemed to be most relevant.,I don't think it evaluated anything well. It missed key words like "puck, hockey and baseball". It still seemed to figure it out sometimes.,It used key words that had been programmed into its search model that were commonly associated with either sport.
_explain,feedback_importance_why,I don't need to provide feedback. I think if it didn't help, I just wouldn't use it.,I think it could made better by incorporating human feedback.,It would only help all involved for it to get better.,To correct errors.,In case there were problems, I could help fix them.,No way to improve if it doesn't know what it gets right and wrong. ,because it did get some wrong. ,I would need to give feedback to help the model identify the key words that served as the best evidence.,Feedback would be needed as far as the keywords used. Some were totally not relevant. So being able to remove words and add words would be very useful.,The key words it used were not very helpful in making a determination of a correct response, they need to be changed.,I feel like I would need to train it more to get things right.,feedback would help me as well as the company to see if anything need to be done to correct the problem,Because of everything I stated above -- there needs to be a way to improve the model's performance,If I could give feedback to improve this model, I would want to be able to give it my own useful baseball and hockey terms instead of it using its own.,It currently needs feedback/guidance to be effective. right now it might as well be a see-and-say spinning around an making random choices,I think feedback is always important, especially when it comes to situations like this. There's always room for improvement.,This model needs work to become usable. How will the developers know what to fix or improve without feedback from users?,Because I would want to be able to help it improve,That way, any errors or imperfections could hopefully be ironed out so to speak.,I would want better keyword inputs.,Because that is the best way to improve it and make it more accurate which is what I would be looking for.,I mean...sure, I guess. I'd like to provide feedback if it would be helpful.,We should be able to train it like if we tell it that it's wrong based on this information then it should re-sort it and know for better.,I would have to have a way to improve it because the methodology seems somewhat flawed,Maybe feedback would help the model to be more accurate.,The model seemed to miss obvious words in some of the emails.,If a feedback options was intuitive and efficient I would like this option.  If it's cumbersome and complicated it could significantly slow workflow.,It needs improvement.,It should have the ability to pick up key words. I would ask it to sort based on names and keywords etc.,If I am susing it, chances are developers are not going to change it once it hist market.
_explain,frustration_why,Well I would want everything to be completely accurate, thus I would feel like I was basically "cleaning up" after this thing if I used it.,It seemed to make quite a few errors in categorizing.,It's not a task that I think requires 100% accuracy because of its importance doesn't seem too high. It can also be corrected manually pretty effortlessly when it makes a mistake.,There is too many errors. ,It was correct most of the time and it would save time.,It would eventually learn to do better.,because I would rather be able to check stuff and make sure it is correctly going out the way it should. ,I would not feel comfortable relying on the model's predictions because of the seemingly random words it decided were most important. Also, the model made several mistakes identifying emails that contained good clues.,I don't think that I'd be frustrated. For teh most part it did an ok job of sorting the emails.,It would make way too many mistakes to be acceptable.,It was very inaccurate and used bad keywords for categorization.,I see this as a learning tool and is faster and will help save time,There were too many errors -- at least according to my standards,This model did get quite a few of the emails wrong. So I would be frustrated not to give my boss good work.,I feel like it's randomly selecting words and then making a random decision from those selected words. Two random's don't make a right,I think the model is okay. There were some errors but overall I was happy with it. ,This model made substantially more mistakes than an adult human fluent in English might be expected to make. I would have to doublecheck its work, which would not end up saving me time at all. ,It was incorrect more than it was right,I feel that it is flawed. Just because of three words that it thinks is important to determine in the results. ,It would be much easier than sorting through them myself. I could rely on it for 90% of the emails.,Because I think that it would miss some of the obvious picks and I would almost feel that I would have to double-check them all. If I had to do that then I may as well have just done it myself in the first place.,I would definitely feel like I needed to double check everything, so I might as well do it myself the first time. ,I would feel ashamed and frustrated if my emails were sorted incorrectly.,The machine is not accurate enough,The margin of error was too big. I would have to check the work of the model.,It mostly chose the correct sport.,It would take at least a portion of the workload away.,I believe the model needs a much stronger keyword database to determine how to sort the emails. Most of the time, the highlighted words were not the best words to determine the subject of the emails. The words in the Subject line of the email should be highly weighted. Words like team names and league names don't seem to have a higher priority over other words. Action words like pitch, run, block, skate, etc. should be priorities as well. Word pairs should always be considered.,It doesn't pick up key words.,I noticed errors that were blatant and could be figured out by a human much more efficiently.
_explain,learn_why,I think it learned if it saw what I chose as the correct answer. I think it could possibly reevaluate to see what it was missing.,I'm not sure either way.,I don't know if that's how it learns or not.,I'm not sure, I would have to continue using it.,It's programmed to learn.,That's how they learn. They make a guess and you tell them right or wrong. ,because maybe over time it will learn there was something else showing that it was hockey vs baseball,Since the highlighted words were seemingly random and since there were a few mistakes throughout the 20 emails I reviewed, I do not believe it learned or improved.,The keywords seemed to be a little better towards the end, but not by much. more learning is needed.,Too many errors and it seemed to be getting worse.,It might have, but could definitely use a larger sample size to get more accurate.,It was sorted just fine for me and easy to read through the emaail,No, because the errors kept being made,The yellow words in learned from were not always sports related, which is why it could not correctly identify the emails each time.,There was 0 improvement throughout the course of the emails, and there was obviously no feedback, so it couldn't course correct.,I'm not really sure if it learned but I think that it was pretty accurate for the most part. ,Isn't that why this task is called "machine learning," because the model is learning? It didn't learn enough fast enough to substantially improve within the set of 20 emails. I suspect it would take a great many emails before the model's performance improved enough to make it usable.,It did not seem to get more accurate with time,I really could not answer that honestly. I hope that it did.,There were still mistakes but it was a good amount of correct versus wrong.,I did not see anything to make me think that it did but I really was not looking for any pattern to accurately answer this question.,I don't really know. If it was programmed to look for specific words and that's what it did then I'm not sure how it could learn unless it was programmed to also learn from it's mistakes.,I'm not sure whether it learned from the  20 emails. ,Not really, as I saw not evidence of improvement.,How could it learn when there wasn't a way to provide feedback?,I'm not sure.,The model should learn from the continued verification of data, but it's presumptions of me to assume the model is actually learning.  I don't remember it being mentioned for sure.,I didn't get a since of improved accuracy over the course of the 20 emails.,I am not sure.,The key words it was using to sort with were not always ones that could easily describe a sport.
_explain,accuracy_standard_why,I think it is acceptable just because there are a lot of words that may be used for both sports. For instance, it may not distinguish team names as being related to one sport or the other, and the word "score" could relate to both sports.,Some of the emails are ambiguous and don't have words that point to it being about one sport or the other.,It's hard to expect it to be perfect and it was still accurate 75% of the time it seemed. Most of the times when it wasn't, it was easy to decipher which sport the email was about on my own.,No, the two are pretty easy to distinguish and this should not be making mistakes. ,It's a computer program and there are going to be mistakes.
,That's how it learns.,because it's not perfect it's a machine. ,Rare mistakes are understandable because some emails may not contain any good clues. However, if a human needs to review the model's work, then it is not really a time-saver.,It's understandable that mistakes were made, but some of the mistakes made were inexcusable. (the NHL error being the one that sticks out the most, but there were other bad mistakes made.),Not in this small sample.,No model is perfect but this was highly inaccurate.,nothing is perfect. So, therefore it is acceptable to have some mistakes alone the way,I may be wrong, but to my way of thinking this model should be like a robot and should not make mistakes at all -- that's what humans are for -- lol,Because my boss is depending on me to sort these emails correctly.  So if this machine goes by nonsense words like common verbs and adjectives, it's not going to be able to distinguish where the email was about baseball or hockey very well and make quite a few mistakes, which my boss will not be too happy about.,I do feel it's acceptable to error, nothing and no one is perfect, but the times it was right was more dumb luck than anything.,I think it's okay as long as the error rate is below 10%. Some emails are just ambiguous and unclear. Also, it is challenging to program a machine to differentiate between some terms that need context.  ,I wouldn't expect a model to be perfect - if it made a few more mistakes than a human would make, I would still find it useful as it would still save me time.,It is only analytical and based on an algorithm; it should be more accurate than not,The boss would not likely accept mistakes. ,Some of the emails showed no way to distinguish between sports even with prior knowledge.,It's a job assigned from my boss so I would take it pretty seriously and would be embarrassed to have some mistakes, especially if it's an obvious mistake like some of the emails were that I judged, myself.,Humans make mistakes, so I expect a machine to make a few mistakes, too, especially during a learning phase. However, with time, I would think these would diminish.,I felt the highlighted words weren't always great and there could have been better words that the model should have highlighted.,Some of the mistakes seemed very strange. For example, one email was labeled baseball when the word hockey was in the email itself. I do not see how such a mistake was made.,I think the model made too many mistakes. I believe it made about 4 mistakes out of the 20. That's too high of a percentage.,It may make a few mistakes initially, but will make fewer mistakes as it learns what words to look for.,When dealing with English and special symbols I would assume that the AI would occasional come across items that have not been learned yet.,With thousands of emails to sort, it would take just as much time to filter out mistakes as it would take to manually sort the emails without mistakes.,I think it should have picked up more key words.,Because some of the mistakes I witnessed indicated the margin of error had the potential to be too high to be considered reliable or effective.
_explain,overall,I like it! I think it is pretty good at sorting out words that could speed the process up, but I would never rely on it entirely. There are context in human thoughts and writing that only other humans are able to sort.,I thought some of the words it used to categorize the emails weren't very helpful and that caused it to make mistakes.,Overall I found it more useful than not and could see it improving to having almost complete accuracy.,I think it did an adequate, but not great, job.,It was simple.,Seems like a great model that's moving in the right direction.,It was cool; I would just be worried that it wasn't perfect and worried it might miss something. ,The model was only slightly helpful since I needed to review each email to discover if the prediction was accurate.,For the most part it did a good job. I just wish i could understand it's logic clearly. With some work I can see this model doing a good job.,I would say this model failed and needs to be reworked.,It was mediocre, could definitely use improvement.,I feel good about the machine. And it would be helpful to have it available for usage.,This hit (experiment) was fun, but using the model was a bit frustrating. I am somewhat of a perfectionist, though, so that may just be me.,I wasn't too happy with it.  I felt like it didn't pick up on the sports vocabulary like I thought it should.,I think I have REPEATEDLY and clearly made my point.
To sum up, it's random and inefficient.,I liked using the model. I felt calm and relax while reading the emails. ,It was pretty interesting. I had no idea that tools like this existed; I learned something new today.,I did not find it useful at all. The keywords it highlighted were not even relevant to either sport. Sometimes the email would contain the exact word "hockey" or "baseball", yet the model only highlighted other words.,Honestly, I thought it was interesting. I think there were a few (2 or so) that I myself was unsure of, but the majority of them I could tell by reading them myself.,IT was simple and easy to understand.,It was interesting but a little surprising with some of the keywords it chose to use for judgment.,I thought it was interesting, but I was confused as to how some of the words were selected.,I think it would be nice if it was accurate maybe 95% of the time, but I didn't feel like it was.,I was not very impressed. Too many emails were wrongly labeled and too many completely innocuous and irrelevant words were highlighted.,I couldn't figure out its pattern.,It was fun.,I feel this model needs work but could evolve into a useful tool.,The model is a good start, but it would not actually solve the problem. There would still have to be a hands-on quality check for errors.,It was just ok but needs improvement.,It as average and I was left unimpressed and not dubious of its ability to accurately sort common and obvious information.
_explain,perf_why,I think it may be able to distinguish things a bit differently after seeing how they were to be properly sorted. It would reevaluate what key words to search for.,I think it may have been incorporating my feedback.,Again, not sure how well the model can learn on its own and how much it relies on being manually reprogrammed. ,I'm not sure if the model learns from more usage.,These programs get better as they function and learn algorithms ,It would get better with every set.,because it now has more words to add to it's database. ,The words it highlighted seemed random; it continued to make a few mistakes and failed to identify important word clues.,I think the model with some adjustments and time could sort these emails easily. Given the slight keyword improvement i think the model would improve a bit on the next set of emails. If given a better keyword list, the model can improve dramatically. ,It kept making errors on the emails that were easily baseball or hockey.,Small sample size.,It would categorise the email the same way,I don't know if the model got feedback or not,If it still used the same words to try to identify the correct sports emails, than it would still make the same amount of errors.,Again 0 improvement. Similar chances of randomly being correct and incorrect.,I made sure that I categorized each email to the best of my abilities. I don't think I made any small errors. I hope that the email did learn and improve itself. ,I don't think 40 is a large enough sample size to substantially affect the model's behavior.,It never improved or got worse over time,I chose about the same to give it the benefit of the doubt. ,I'm not sure if the bot was learning or not.,I'm mostly thinking that whoever made this model would write in the code to help it learn and become more accurate.,Assuming it could learn from it's mistakes, I would think it would do better. If it doesn't have this ability then I'd say it would do the same.,I think models should be able to learn. I notice that it sorts a lot of the same word.,If it continued to focus on irrelevant words I do not see how it would improve.,I didn't see anyway to provide feedback so the model could learn from its mistakes.,Some of the words marked as important were just general words.,The more consensus it achieved the higher the level of confidence it will have. In my mind.,I didn't get the sense that there was a learning mechanism in the model. It seemed more like a basic text search of the email text. Many of the highlighted words had no relevance to what was being sorted.,I hope it would learn and do it better.,For the above stated reasons, because of the tools it has at its disposal put into place by its developers.
_explain,trust_why,I would trust it, but only as a tool to help me sort faster. I would not rely on it entirely because I think it lacks the human touch that is able to distinguish things.,I don't think it was accurate enough for me to use without monitoring it.,It seemed to be right 75% of the time so I somewhat agree that it'd trust it. I would obviously not trust it fully though and expect mistakes here and there.,There is too many errors. ,It was correct most of the time.,There are so many emails he probably wouldn't notice any mistakes. ,because it was wrong a couple times,Although the model was fairly accurate, it missed too many emails that contained sufficient evidence to make a judgment accurately, and the words it identified as the best clues were seemingly random.,Due to the poor mistakes, I would have to double check the work done by this model.,It made too many mistakes even in this small sample size.,It was very inaccurate and used bad keywords for categorization.,it seems to be a good model to use. And I will trust it,Again, too many errors,This model incorrectly identified more than a few emails, so I wouldn't trust it to do the work.,again, I feel like it did a poor job. I'll bring up random again, I feel like a broken record about this,There were some errors that were easy to detect. I think that the model would have to be refined a bit more to be used by my boss.,The model needs more work in order to raise its accuracy level. If it no longer miscategorized emails that actually contained the word "hockey" or "baseball," for example, I might be willing to use it then,It made too many mistakes,Too many possibilities for errors.,It did it relatively accurately, even humans may make mistakes.,Because of some of the obvious errors that I saw as I was reading them and the choices that the model made that were plainly wrong.,I wouldn't trust it to not make errors, so I'd have to check everything again. ,I didn't feel like the model was right most of them.,The machine often puts irrelevant words in yellow, while completely ignoring relevant words.,The model got too many wrong in my estimation.,It was about 85% accurate,I would not rely entirely on the model at this point depending on the importance of accuracy to by boss.,The criteria used to analyze the words is not thorough enough.,It doesn't pick up key words that would help.,I saw errors that were too obvious.
